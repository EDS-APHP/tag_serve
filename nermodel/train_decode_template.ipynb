{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import train, data_initialization, build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model on conll2003 shared task training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2train = '../conll2003/train.conll2003'\n",
    "path2dev = '../conll2003/dev.conll2003'\n",
    "path2test = '../conll2003/test.conll2003'\n",
    "path2model = '../pretrained/myModel/myModel'\n",
    "modelDir = os.path.join(*path2model.split('/')[:-1])\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "path2emb = '../pretrained/glove.6B.50d.txt'\n",
    "confdict = {# IO\n",
    "            'train_dir':path2train,\n",
    "            'dev_dir':path2dev,\n",
    "            'test_dir':path2test,\n",
    "            'model_dir':path2model,\n",
    "            # Embeddings\n",
    "            'word_emb_dir':path2emb,\n",
    "            'char_emb_dir':None,\n",
    "            'word_emb_dim':50,\n",
    "            'char_emb_dim':30,\n",
    "            # Network\n",
    "            'use_crf':True,\n",
    "            'use_char':True,\n",
    "            'use_feats': False,\n",
    "            'word_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            'char_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            # HP\n",
    "            'HP_cnn_layer':4 ,\n",
    "            'HP_char_hidden_dim':50,\n",
    "            'HP_hidden_dim':200,\n",
    "            'HP_dropout':0.5,\n",
    "            'HP_lstm_layer':1,\n",
    "            'HP_bilstm':True,\n",
    "            'HP_lr':0.015,\n",
    "            # training\n",
    "            'optimizer':'SGD',\n",
    "            'batch_size':10,\n",
    "            'iteration':5\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained word embedding, norm False, dir: ../pretrained/glove.6B.50d.txt\n",
      "Embedding: \n",
      " pretrain words: 400000, perfect_match: 14618, case_match: 11722, oov: 3951\n"
     ]
    }
   ],
   "source": [
    "# initialization of data object and training (equivalent to main.myTrain(confdict))\n",
    "\n",
    "data = Data()\n",
    "data.read_config(confdict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data_initialization(data)\n",
    "data.generate_instance('train')\n",
    "data.generate_instance('dev')\n",
    "data.generate_instance('test')\n",
    "data.build_pretrain_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "****************************************\n",
      "----------Data summary:----------\n",
      "\n",
      " HP_gpu: False\n",
      " MAX_SENTENCE_LENTGH: 1000\n",
      " number_normalized: False\n",
      " word_alphabet: 30292\n",
      " char_alphabet_size: 87\n",
      " label_alphabet_size: 10\n",
      " load_model_dir: None\n",
      "\n",
      "\n",
      "I/O:\n",
      " tagScheme: BIO\n",
      " train_dir: ../conll2003/train.conll2003\n",
      " dev_dir: ../conll2003/dev.conll2003\n",
      " test_dir: ../conll2003/test.conll2003\n",
      " raw_dir: None\n",
      " dset_dir: None\n",
      " word_emb_dir: ../pretrained/glove.6B.50d.txt\n",
      " char_emb_dir: None\n",
      " feature_emb_dirs: []\n",
      "\n",
      "\n",
      "Network:\n",
      " word_feature_extractor: LSTM\n",
      " use_char: True\n",
      " char_feature_extractor: LSTM\n",
      " use_crf: True\n",
      "\n",
      "\n",
      "Network Hyperparameters:\n",
      " word_emb_dim: 50\n",
      " char_emb_dim: 30\n",
      " feature_emb_dims: []\n",
      " HP_char_hidden_dim: 50\n",
      " HP_hidden_dim: 200\n",
      " HP_lstm_layer: 1\n",
      " HP_bilstm: True\n",
      " HP_cnn_layer: 4\n",
      " HP_dropout: 0.5\n",
      "\n",
      "\n",
      "Training Hyperparameters:\n",
      " average_batch_loss: False\n",
      " optimizer: SGD\n",
      " iteration: 5\n",
      " batch_size: 10\n",
      " HP_lr: 0.015\n",
      " HP_lr_decayr: 0.05\n",
      " HP_clip: None\n",
      " HP_momentum: 0\n",
      " HP_l2: 1e-08\n",
      "****************************************\n",
      "\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  LSTM\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "build char sequence feature extractor: LSTM ...\n",
      "build CRF...\n",
      "Epoch 0/5\n",
      " Learning rate is setted as: 0.015\n",
      " Instance 500; Time 3.8s; loss 4.774e+03; acc 5901.0/7131.0=0.8275\n",
      " Instance 1000; Time 3.6s; loss 2.95e+03; acc 11440.0/13626.0=0.8396\n",
      " Instance 1500; Time 5.1s; loss 2.941e+03; acc 17759.0/20929.0=0.8485\n",
      " Instance 2000; Time 5.3s; loss 2.335e+03; acc 23445.0/27437.0=0.8545\n",
      " Instance 2500; Time 4.7s; loss 2.178e+03; acc 29320.0/34104.0=0.8597\n",
      " Instance 3000; Time 4.8s; loss 1.98e+03; acc 34983.0/40519.0=0.8634\n",
      " Instance 3500; Time 3.8s; loss 1.611e+03; acc 40871.0/47010.0=0.8694\n",
      " Instance 4000; Time 3.9s; loss 1.454e+03; acc 47460.0/54161.0=0.8763\n",
      " Instance 4500; Time 3.7s; loss 1.241e+03; acc 53750.0/60950.0=0.8819\n",
      " Instance 5000; Time 3.5s; loss 1.236e+03; acc 59649.0/67316.0=0.8861\n",
      " Instance 5500; Time 3.6s; loss 1.136e+03; acc 65816.0/73936.0=0.8902\n",
      " Instance 6000; Time 3.4s; loss 1.1e+03; acc 72047.0/80621.0=0.8937\n",
      " Instance 6500; Time 3.8s; loss 1.064e+03; acc 78770.0/87783.0=0.8973\n",
      " Instance 7000; Time 3.8s; loss 949.2; acc 85310.0/94712.0=0.9007\n",
      " Instance 7500; Time 4.7s; loss 987.6; acc 91776.0/101583.0=0.9035\n",
      " Instance 8000; Time 4.9s; loss 1.111e+03; acc 98149.0/108417.0=0.9053\n",
      " Instance 8500; Time 4.3s; loss 955.4; acc 104309.0/114948.0=0.9074\n",
      " Instance 9000; Time 5.1s; loss 908.7; acc 110507.0/121515.0=0.9094\n",
      " Instance 9500; Time 4.9s; loss 957.6; acc 117037.0/128444.0=0.9112\n",
      " Instance 10000; Time 5.1s; loss 907.0; acc 123907.0/135662.0=0.9134\n",
      " Instance 10500; Time 4.7s; loss 900.9; acc 129868.0/142008.0=0.9145\n",
      " Instance 11000; Time 5.0s; loss 910.6; acc 136380.0/148890.0=0.916\n",
      " Instance 11500; Time 4.1s; loss 904.1; acc 143133.0/156022.0=0.9174\n",
      " Instance 12000; Time 3.6s; loss 827.8; acc 149526.0/162735.0=0.9188\n",
      " Instance 12500; Time 3.9s; loss 827.0; acc 156643.0/170206.0=0.9203\n",
      " Instance 13000; Time 4.5s; loss 858.9; acc 163570.0/177502.0=0.9215\n",
      " Instance 13500; Time 4.6s; loss 834.2; acc 170136.0/184418.0=0.9226\n",
      " Instance 14000; Time 4.3s; loss 814.2; acc 177057.0/191649.0=0.9239\n",
      " Instance 14500; Time 3.5s; loss 691.6; acc 183482.0/198355.0=0.925\n",
      " Instance 14986; Time 3.4s; loss 689.8; acc 189410.0/204566.0=0.9259\n",
      " Epoch: 0 training finished. Time: 1.3e+02s; speed: 1.2e+02st/s; total loss: 41035.19400024414\n",
      "gold_num = 5913; predict_num = 5944; right_num = 5063\n",
      "Dev: time: 7.5s, speed 4.7e+02st/s; acc: 0.9711, p: 0.8518, r: 0.8562, f: 0.854\n",
      "gold_num = 5596; predict_num = 5696; right_num = 4663\n",
      "Test: time: 7.5s, speed 5.4e+02st/s; acc: 0.9634, p: 0.8186, r: 0.8333, f: 0.8259\n",
      "\"Exceed previous best f score: -10\n",
      "Save current best model in file: ../pretrained/myModel/myModel.0.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 1/5\n",
      " Learning rate is setted as: 0.014285714285714285\n",
      " Instance 500; Time 3.6s; loss 801.3; acc 6447.0/6795.0=0.9488\n",
      " Instance 1000; Time 3.9s; loss 824.0; acc 13520.0/14244.0=0.9492\n",
      " Instance 1500; Time 3.6s; loss 703.9; acc 20238.0/21246.0=0.9526\n",
      " Instance 2000; Time 4.9s; loss 785.8; acc 26820.0/28195.0=0.9512\n",
      " Instance 2500; Time 4.2s; loss 704.4; acc 33462.0/35142.0=0.9522\n",
      " Instance 3000; Time 3.5s; loss 766.3; acc 39592.0/41600.0=0.9517\n",
      " Instance 3500; Time 3.7s; loss 727.0; acc 46216.0/48515.0=0.9526\n",
      " Instance 4000; Time 3.6s; loss 715.3; acc 52701.0/55291.0=0.9532\n",
      " Instance 4500; Time 3.6s; loss 742.2; acc 59044.0/61972.0=0.9528\n",
      " Instance 5000; Time 3.4s; loss 639.6; acc 65348.0/68533.0=0.9535\n",
      " Instance 5500; Time 3.8s; loss 765.3; acc 72527.0/76045.0=0.9537\n",
      " Instance 6000; Time 3.7s; loss 673.0; acc 78939.0/82742.0=0.954\n",
      " Instance 6500; Time 3.6s; loss 654.9; acc 85562.0/89642.0=0.9545\n",
      " Instance 7000; Time 3.7s; loss 717.0; acc 91774.0/96168.0=0.9543\n",
      " Instance 7500; Time 3.5s; loss 632.0; acc 98288.0/102932.0=0.9549\n",
      " Instance 8000; Time 3.6s; loss 641.1; acc 104698.0/109603.0=0.9552\n",
      " Instance 8500; Time 3.4s; loss 622.9; acc 110651.0/115803.0=0.9555\n",
      " Instance 9000; Time 4.8s; loss 717.0; acc 117125.0/122568.0=0.9556\n",
      " Instance 9500; Time 4.6s; loss 647.8; acc 123435.0/129140.0=0.9558\n",
      " Instance 10000; Time 3.6s; loss 676.1; acc 129580.0/135574.0=0.9558\n",
      " Instance 10500; Time 3.6s; loss 672.5; acc 136289.0/142540.0=0.9561\n",
      " Instance 11000; Time 3.8s; loss 600.0; acc 143381.0/149891.0=0.9566\n",
      " Instance 11500; Time 3.9s; loss 667.4; acc 150548.0/157331.0=0.9569\n",
      " Instance 12000; Time 3.6s; loss 558.9; acc 157128.0/164144.0=0.9573\n",
      " Instance 12500; Time 4.1s; loss 670.6; acc 163149.0/170440.0=0.9572\n",
      " Instance 13000; Time 3.6s; loss 644.3; acc 169874.0/177433.0=0.9574\n",
      " Instance 13500; Time 3.6s; loss 578.9; acc 176505.0/184291.0=0.9578\n",
      " Instance 14000; Time 4.3s; loss 659.1; acc 182916.0/190984.0=0.9578\n",
      " Instance 14500; Time 4.7s; loss 615.5; acc 189646.0/198005.0=0.9578\n",
      " Instance 14986; Time 4.0s; loss 735.4; acc 195897.0/204566.0=0.9576\n",
      " Epoch: 1 training finished. Time: 1.2e+02s; speed: 1.3e+02st/s; total loss: 20559.361907958984\n",
      "gold_num = 5913; predict_num = 5859; right_num = 5267\n",
      "Dev: time: 7.8s, speed 4.5e+02st/s; acc: 0.9784, p: 0.899, r: 0.8907, f: 0.8948\n",
      "gold_num = 5596; predict_num = 5552; right_num = 4828\n",
      "Test: time: 7.8s, speed 5.4e+02st/s; acc: 0.9711, p: 0.8696, r: 0.8628, f: 0.8662\n",
      "\"Exceed previous best f score: 0.8540102892805937\n",
      "Save current best model in file: ../pretrained/myModel/myModel.1.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 2/5\n",
      " Learning rate is setted as: 0.013636363636363634\n",
      " Instance 500; Time 3.6s; loss 688.0; acc 6484.0/6776.0=0.9569\n",
      " Instance 1000; Time 3.6s; loss 630.7; acc 12986.0/13553.0=0.9582\n",
      " Instance 1500; Time 3.7s; loss 468.2; acc 19624.0/20397.0=0.9621\n",
      " Instance 2000; Time 4.2s; loss 501.9; acc 25584.0/26571.0=0.9629\n",
      " Instance 2500; Time 3.9s; loss 600.3; acc 32324.0/33572.0=0.9628\n",
      " Instance 3000; Time 3.6s; loss 573.8; acc 38806.0/40289.0=0.9632\n",
      " Instance 3500; Time 3.8s; loss 568.8; acc 45262.0/46974.0=0.9636\n",
      " Instance 4000; Time 3.6s; loss 509.3; acc 51691.0/53629.0=0.9639\n",
      " Instance 4500; Time 3.9s; loss 628.6; acc 58465.0/60662.0=0.9638\n",
      " Instance 5000; Time 3.6s; loss 552.4; acc 65069.0/67515.0=0.9638\n",
      " Instance 5500; Time 4.0s; loss 557.5; acc 71484.0/74167.0=0.9638\n",
      " Instance 6000; Time 5.2s; loss 586.1; acc 78030.0/80967.0=0.9637\n",
      " Instance 6500; Time 5.0s; loss 572.9; acc 84456.0/87629.0=0.9638\n",
      " Instance 7000; Time 4.0s; loss 637.3; acc 91071.0/94507.0=0.9636\n",
      " Instance 7500; Time 5.0s; loss 676.6; acc 97605.0/101341.0=0.9631\n",
      " Instance 8000; Time 5.0s; loss 587.9; acc 104584.0/108565.0=0.9633\n",
      " Instance 8500; Time 5.0s; loss 538.4; acc 111280.0/115469.0=0.9637\n",
      " Instance 9000; Time 4.6s; loss 635.8; acc 118230.0/122690.0=0.9636\n",
      " Instance 9500; Time 3.6s; loss 691.4; acc 124935.0/129684.0=0.9634\n",
      " Instance 10000; Time 4.3s; loss 529.6; acc 131398.0/136361.0=0.9636\n",
      " Instance 10500; Time 4.0s; loss 551.3; acc 138192.0/143402.0=0.9637\n",
      " Instance 11000; Time 3.8s; loss 545.7; acc 144435.0/149875.0=0.9637\n",
      " Instance 11500; Time 4.2s; loss 576.7; acc 150866.0/156551.0=0.9637\n",
      " Instance 12000; Time 3.8s; loss 500.1; acc 157562.0/163437.0=0.9641\n",
      " Instance 12500; Time 5.5s; loss 552.0; acc 164205.0/170303.0=0.9642\n",
      " Instance 13000; Time 4.9s; loss 510.2; acc 170699.0/177009.0=0.9644\n",
      " Instance 13500; Time 4.5s; loss 568.0; acc 177542.0/184088.0=0.9644\n",
      " Instance 14000; Time 4.8s; loss 582.9; acc 184032.0/190824.0=0.9644\n",
      " Instance 14500; Time 5.1s; loss 619.8; acc 190713.0/197775.0=0.9643\n",
      " Instance 14986; Time 5.0s; loss 548.6; acc 197271.0/204566.0=0.9643\n",
      " Epoch: 2 training finished. Time: 1.3e+02s; speed: 1.2e+02st/s; total loss: 17290.782348632812\n",
      "gold_num = 5913; predict_num = 5938; right_num = 5314\n",
      "Dev: time: 7.5s, speed 4.6e+02st/s; acc: 0.9794, p: 0.8949, r: 0.8987, f: 0.8968\n",
      "gold_num = 5596; predict_num = 5667; right_num = 4872\n",
      "Test: time: 7.5s, speed 5.4e+02st/s; acc: 0.9706, p: 0.8597, r: 0.8706, f: 0.8651\n",
      "\"Exceed previous best f score: 0.8948352021746517\n",
      "Save current best model in file: ../pretrained/myModel/myModel.2.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 3/5\n",
      " Learning rate is setted as: 0.013043478260869566\n",
      " Instance 500; Time 4.8s; loss 502.2; acc 6675.0/6882.0=0.9699\n",
      " Instance 1000; Time 5.6s; loss 675.2; acc 13145.0/13628.0=0.9646\n",
      " Instance 1500; Time 5.6s; loss 480.5; acc 19870.0/20568.0=0.9661\n",
      " Instance 2000; Time 4.9s; loss 558.0; acc 26265.0/27212.0=0.9652\n",
      " Instance 2500; Time 4.8s; loss 513.4; acc 32614.0/33783.0=0.9654\n",
      " Instance 3000; Time 5.0s; loss 538.5; acc 39593.0/40980.0=0.9662\n",
      " Instance 3500; Time 5.0s; loss 495.8; acc 45895.0/47490.0=0.9664\n",
      " Instance 4000; Time 4.9s; loss 528.4; acc 52336.0/54157.0=0.9664\n",
      " Instance 4500; Time 5.1s; loss 565.6; acc 59253.0/61302.0=0.9666\n",
      " Instance 5000; Time 4.7s; loss 511.5; acc 65779.0/68035.0=0.9668\n",
      " Instance 5500; Time 4.7s; loss 510.6; acc 71988.0/74442.0=0.967\n",
      " Instance 6000; Time 5.7s; loss 499.9; acc 78647.0/81310.0=0.9672\n",
      " Instance 6500; Time 5.1s; loss 579.7; acc 85321.0/88235.0=0.967\n",
      " Instance 7000; Time 4.1s; loss 519.5; acc 91867.0/95017.0=0.9668\n",
      " Instance 7500; Time 3.7s; loss 467.3; acc 98482.0/101874.0=0.9667\n",
      " Instance 8000; Time 3.7s; loss 529.8; acc 104840.0/108463.0=0.9666\n",
      " Instance 8500; Time 4.1s; loss 473.8; acc 111599.0/115415.0=0.9669\n",
      " Instance 9000; Time 5.4s; loss 515.1; acc 118187.0/122203.0=0.9671\n",
      " Instance 9500; Time 5.0s; loss 496.3; acc 124468.0/128708.0=0.9671\n",
      " Instance 10000; Time 3.7s; loss 518.6; acc 131095.0/135567.0=0.967\n",
      " Instance 10500; Time 4.2s; loss 518.4; acc 138095.0/142776.0=0.9672\n",
      " Instance 11000; Time 3.5s; loss 533.7; acc 144398.0/149302.0=0.9672\n",
      " Instance 11500; Time 4.0s; loss 572.2; acc 151107.0/156242.0=0.9671\n",
      " Instance 12000; Time 4.8s; loss 451.7; acc 158074.0/163386.0=0.9675\n",
      " Instance 12500; Time 5.4s; loss 450.3; acc 164671.0/170189.0=0.9676\n",
      " Instance 13000; Time 5.2s; loss 443.0; acc 171446.0/177168.0=0.9677\n",
      " Instance 13500; Time 5.2s; loss 562.8; acc 178317.0/184284.0=0.9676\n",
      " Instance 14000; Time 4.8s; loss 466.2; acc 184651.0/190809.0=0.9677\n",
      " Instance 14500; Time 4.9s; loss 485.9; acc 191497.0/197856.0=0.9679\n",
      " Instance 14986; Time 4.7s; loss 494.5; acc 198002.0/204566.0=0.9679\n",
      " Epoch: 3 training finished. Time: 1.4e+02s; speed: 1.1e+02st/s; total loss: 15458.491455078125\n",
      "gold_num = 5913; predict_num = 5918; right_num = 5374\n",
      "Dev: time: 7.5s, speed 4.7e+02st/s; acc: 0.9818, p: 0.9081, r: 0.9088, f: 0.9085\n",
      "gold_num = 5596; predict_num = 5624; right_num = 4918\n",
      "Test: time: 7.5s, speed 4.7e+02st/s; acc: 0.9733, p: 0.8745, r: 0.8788, f: 0.8766\n",
      "\"Exceed previous best f score: 0.8968019576407054\n",
      "Save current best model in file: ../pretrained/myModel/myModel.3.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 4/5\n",
      " Learning rate is setted as: 0.0125\n",
      " Instance 500; Time 4.9s; loss 572.5; acc 6635.0/6872.0=0.9655\n",
      " Instance 1000; Time 4.9s; loss 438.9; acc 13361.0/13794.0=0.9686\n",
      " Instance 1500; Time 4.6s; loss 422.7; acc 19946.0/20547.0=0.9707\n",
      " Instance 2000; Time 3.6s; loss 503.0; acc 26290.0/27122.0=0.9693\n",
      " Instance 2500; Time 3.5s; loss 542.1; acc 32918.0/33950.0=0.9696\n",
      " Instance 3000; Time 3.5s; loss 510.1; acc 39288.0/40547.0=0.9689\n",
      " Instance 3500; Time 3.7s; loss 514.9; acc 45839.0/47313.0=0.9688\n",
      " Instance 4000; Time 3.6s; loss 456.8; acc 52150.0/53825.0=0.9689\n",
      " Instance 4500; Time 3.8s; loss 447.0; acc 58451.0/60308.0=0.9692\n",
      " Instance 5000; Time 3.8s; loss 421.6; acc 65098.0/67120.0=0.9699\n",
      " Instance 5500; Time 3.7s; loss 463.2; acc 71773.0/73995.0=0.97\n",
      " Instance 6000; Time 4.0s; loss 416.9; acc 78388.0/80775.0=0.9704\n",
      " Instance 6500; Time 4.3s; loss 519.8; acc 85028.0/87628.0=0.9703\n",
      " Instance 7000; Time 3.8s; loss 454.9; acc 91552.0/94338.0=0.9705\n",
      " Instance 7500; Time 3.8s; loss 541.9; acc 98250.0/101255.0=0.9703\n",
      " Instance 8000; Time 3.7s; loss 445.4; acc 104616.0/107820.0=0.9703\n",
      " Instance 8500; Time 3.6s; loss 461.1; acc 111385.0/114768.0=0.9705\n",
      " Instance 9000; Time 3.6s; loss 523.2; acc 117890.0/121511.0=0.9702\n",
      " Instance 9500; Time 3.9s; loss 472.2; acc 124492.0/128288.0=0.9704\n",
      " Instance 10000; Time 4.4s; loss 537.3; acc 131382.0/135380.0=0.9705\n",
      " Instance 10500; Time 5.0s; loss 439.7; acc 138149.0/142345.0=0.9705\n",
      " Instance 11000; Time 3.8s; loss 466.7; acc 144937.0/149324.0=0.9706\n",
      " Instance 11500; Time 3.9s; loss 455.3; acc 151765.0/156349.0=0.9707\n",
      " Instance 12000; Time 3.8s; loss 499.8; acc 158387.0/163176.0=0.9707\n",
      " Instance 12500; Time 4.3s; loss 494.9; acc 165186.0/170192.0=0.9706\n",
      " Instance 13000; Time 3.6s; loss 471.3; acc 172129.0/177351.0=0.9706\n",
      " Instance 13500; Time 3.5s; loss 377.9; acc 178482.0/183855.0=0.9708\n",
      " Instance 14000; Time 4.3s; loss 508.7; acc 185673.0/191275.0=0.9707\n",
      " Instance 14500; Time 3.9s; loss 456.3; acc 192002.0/197798.0=0.9707\n",
      " Instance 14986; Time 3.8s; loss 404.8; acc 198608.0/204566.0=0.9709\n",
      " Epoch: 4 training finished. Time: 1.2e+02s; speed: 1.3e+02st/s; total loss: 14240.621490478516\n",
      "gold_num = 5913; predict_num = 5933; right_num = 5426\n",
      "Dev: time: 7.5s, speed 4.7e+02st/s; acc: 0.983, p: 0.9145, r: 0.9176, f: 0.9161\n",
      "gold_num = 5596; predict_num = 5642; right_num = 4976\n",
      "Test: time: 7.5s, speed 4.6e+02st/s; acc: 0.9749, p: 0.882, r: 0.8892, f: 0.8856\n",
      "\"Exceed previous best f score: 0.9084608232609247\n",
      "Save current best model in file: ../pretrained/myModel/myModel.4.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Training done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9160898193483031"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to decode a new input from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model weights from file ../pretrained/myModel/myModel.1.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/myModel/myModel.xpt'\n",
    "path2model = '../pretrained/myModel/myModel.1.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.load_model_dir = path2model\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2write = out_folder + os.path.basename(os.path.splitext(file_name)[0]) + '.out'\n",
    "# open and return the text of the file\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.091 s\n",
      "Decoding speed: 319.2 st/s\n",
      "[['B-ORG', 'O', 'B-PER', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O']]\n",
      "['B-ORG O B-PER O O B-PER O O O B-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O O O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER O B-PER O B-PER O O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O O O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O O O B-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER O B-PER O O O B-PER I-PER O B-PER I-PER O B-PER O O B-PER O B-PER I-PER O B-PER O O B-PER I-PER O\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training informations of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
