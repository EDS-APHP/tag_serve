{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import train, data_initialization, build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model on conll2003 shared task training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2train = '../conll2003/train.conll2003'\n",
    "path2dev = '../conll2003/dev.conll2003'\n",
    "path2test = '../conll2003/test.conll2003'\n",
    "path2model = '../pretrained/myModel/myModel'\n",
    "modelDir = os.path.join(*path2model.split('/')[:-1])\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "path2emb = '../pretrained/glove.6B.50d.txt'\n",
    "confdict = {# IO\n",
    "            'train_dir':path2train,\n",
    "            'dev_dir':path2dev,\n",
    "            'test_dir':path2test,\n",
    "            'model_dir':path2model,\n",
    "            # Embeddings\n",
    "            'word_emb_dir':path2emb,\n",
    "            'char_emb_dir':None,\n",
    "            'word_emb_dim':50,\n",
    "            'char_emb_dim':30,\n",
    "            # Network\n",
    "            'use_crf':True,\n",
    "            'use_char':True,\n",
    "            'use_feats': False,\n",
    "            'word_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            'char_feature_extractor':'CNN', # choose CNN/LSTM/GRU\n",
    "            # HP\n",
    "            'HP_cnn_layer':4 ,\n",
    "            'HP_char_hidden_dim':50,\n",
    "            'HP_hidden_dim':200,\n",
    "            'HP_dropout':0.5,\n",
    "            'HP_lstm_layer':1,\n",
    "            'HP_bilstm':True,\n",
    "            'HP_lr':0.015,\n",
    "            # training\n",
    "            'optimizer':'SGD',\n",
    "            'batch_size':10,\n",
    "            'iteration':5\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained word embedding, norm False, dir: ../pretrained/glove.6B.50d.txt\n",
      "Embedding: \n",
      " pretrain words: 400000, perfect_match: 14618, case_match: 11722, oov: 3951\n"
     ]
    }
   ],
   "source": [
    "# initialization of data object and training (equivalent to main.myTrain(confdict))\n",
    "\n",
    "data = Data()\n",
    "data.read_config(confdict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data_initialization(data)\n",
    "data.generate_instance('train')\n",
    "data.generate_instance('dev')\n",
    "data.generate_instance('test')\n",
    "data.build_pretrain_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "****************************************\n",
      "----------Data summary:----------\n",
      "\n",
      " HP_gpu: False\n",
      " MAX_SENTENCE_LENTGH: 1000\n",
      " number_normalized: False\n",
      " word_alphabet: 30292\n",
      " char_alphabet_size: 87\n",
      " label_alphabet_size: 10\n",
      " load_model_dir: None\n",
      "\n",
      "\n",
      "I/O:\n",
      " tagScheme: BIO\n",
      " train_dir: ../conll2003/train.conll2003\n",
      " dev_dir: ../conll2003/dev.conll2003\n",
      " test_dir: ../conll2003/test.conll2003\n",
      " raw_dir: None\n",
      " dset_dir: None\n",
      " word_emb_dir: ../pretrained/glove.6B.50d.txt\n",
      " char_emb_dir: None\n",
      " feature_emb_dirs: []\n",
      "\n",
      "\n",
      "Network:\n",
      " word_feature_extractor: LSTM\n",
      " use_char: True\n",
      " char_feature_extractor: CNN\n",
      " use_crf: True\n",
      "\n",
      "\n",
      "Network Hyperparameters:\n",
      " word_emb_dim: 50\n",
      " char_emb_dim: 30\n",
      " feature_emb_dims: []\n",
      " HP_char_hidden_dim: 50\n",
      " HP_hidden_dim: 200\n",
      " HP_lstm_layer: 1\n",
      " HP_bilstm: True\n",
      " HP_cnn_layer: 4\n",
      " HP_dropout: 0.5\n",
      "\n",
      "\n",
      "Training Hyperparameters:\n",
      " average_batch_loss: False\n",
      " optimizer: SGD\n",
      " iteration: 100\n",
      " batch_size: 100\n",
      " HP_lr: 0.015\n",
      " HP_lr_decayr: 0.05\n",
      " HP_clip: None\n",
      " HP_momentum: 0\n",
      " HP_l2: 1e-08\n",
      "****************************************\n",
      "\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n",
      "Epoch 0/5\n",
      " Learning rate is setted as: 0.015\n",
      " Instance 500; Time 2.7s; loss 5.6e+05; acc 3677.0/6930.0=0.5306\n",
      " Instance 1000; Time 2.8s; loss 3.893e+05; acc 7614.0/13559.0=0.5615\n",
      " Instance 1500; Time 2.9s; loss 1.153e+05; acc 12460.0/20316.0=0.6133\n",
      " Instance 2000; Time 3.0s; loss 2.051e+05; acc 16349.0/26666.0=0.6131\n",
      " Instance 2500; Time 2.6s; loss 2.77e+05; acc 21195.0/33539.0=0.632\n",
      " Instance 3000; Time 2.8s; loss 2.439e+05; acc 26233.0/40344.0=0.6502\n",
      " Instance 3500; Time 3.6s; loss 2.13e+05; acc 31539.0/47491.0=0.6641\n",
      " Instance 4000; Time 3.1s; loss 2.704e+05; acc 35880.0/53945.0=0.6651\n",
      " Instance 4500; Time 2.9s; loss 1.69e+05; acc 40821.0/60924.0=0.67\n",
      " Instance 5000; Time 2.9s; loss 1.636e+05; acc 45626.0/67909.0=0.6719\n",
      " Instance 5500; Time 3.4s; loss 1.515e+05; acc 50400.0/74656.0=0.6751\n",
      " Instance 6000; Time 2.9s; loss 1.04e+05; acc 55294.0/81467.0=0.6787\n",
      " Instance 6500; Time 2.8s; loss 1.303e+05; acc 60014.0/88050.0=0.6816\n",
      " Instance 7000; Time 2.7s; loss 1.832e+05; acc 64593.0/94921.0=0.6805\n",
      " Instance 7500; Time 2.4s; loss 1.388e+05; acc 69115.0/101809.0=0.6789\n",
      " Instance 8000; Time 2.9s; loss 2.073e+05; acc 74166.0/108819.0=0.6816\n",
      " Instance 8500; Time 2.6s; loss 1.295e+05; acc 79486.0/115645.0=0.6873\n",
      " Instance 9000; Time 2.7s; loss 1.278e+05; acc 84262.0/122578.0=0.6874\n",
      " Instance 9500; Time 2.5s; loss 1.4e+05; acc 88952.0/129358.0=0.6876\n",
      " Instance 10000; Time 2.5s; loss 1.815e+05; acc 93477.0/135744.0=0.6886\n",
      " Instance 10500; Time 2.4s; loss 1.195e+05; acc 98518.0/143027.0=0.6888\n",
      " Instance 11000; Time 2.4s; loss 1.489e+05; acc 103357.0/149462.0=0.6915\n",
      " Instance 11500; Time 2.8s; loss 1.172e+05; acc 108010.0/156049.0=0.6922\n",
      " Instance 12000; Time 2.6s; loss 1.395e+05; acc 112996.0/162919.0=0.6936\n",
      " Instance 12500; Time 2.6s; loss 1.43e+05; acc 118214.0/170191.0=0.6946\n",
      " Instance 13000; Time 2.6s; loss 1.201e+05; acc 123142.0/176870.0=0.6962\n",
      " Instance 13500; Time 3.0s; loss 1.162e+05; acc 127895.0/183670.0=0.6963\n",
      " Instance 14000; Time 2.6s; loss 1.378e+05; acc 132630.0/190733.0=0.6954\n",
      " Instance 14500; Time 2.5s; loss 1.828e+05; acc 137603.0/197726.0=0.6959\n",
      " Instance 14986; Time 2.8s; loss 1.142e+05; acc 142721.0/204566.0=0.6977\n",
      " Epoch: 0 training finished. Time: 8.3e+01s; speed: 1.8e+02st/s; total loss: 5439604.322753906\n",
      "gold_num = 189; predict_num = 0; right_num = 0\n",
      "Dev: time: 0.14s, speed 2.5e+04st/s; acc: 0.8021, p: -1.0, r: 0.0, f: -1.0\n",
      "gold_num = 211; predict_num = 0; right_num = 0\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Test: time: 0.14s, speed 1.9e+04st/s; acc: 0.7656, p: -1.0, r: 0.0, f: -1.0\n",
      "\"Exceed previous best f score: -10\n",
      "Save current best model in file: ../pretrained/myModel/myModel.0.model\n",
      "Epoch 1/5\n",
      " Learning rate is setted as: 0.014285714285714285\n",
      " Instance 500; Time 2.6s; loss 9.794e+04; acc 5473.0/6961.0=0.7862\n",
      " Instance 1000; Time 2.4s; loss 9.074e+04; acc 9891.0/13630.0=0.7257\n",
      " Instance 1500; Time 2.6s; loss 9.016e+04; acc 15067.0/20969.0=0.7185\n",
      " Instance 2000; Time 2.7s; loss 1.422e+05; acc 20121.0/27921.0=0.7206\n",
      " Instance 2500; Time 2.8s; loss 1.044e+05; acc 25360.0/34716.0=0.7305\n",
      " Instance 3000; Time 2.8s; loss 1.528e+05; acc 29866.0/41413.0=0.7212\n",
      " Instance 3500; Time 2.9s; loss 1.014e+05; acc 34667.0/48462.0=0.7153\n",
      " Instance 4000; Time 2.4s; loss 1.426e+05; acc 40104.0/55429.0=0.7235\n",
      " Instance 4500; Time 2.7s; loss 1.122e+05; acc 44578.0/61636.0=0.7232\n",
      " Instance 5000; Time 3.3s; loss 1.577e+05; acc 49283.0/68582.0=0.7186\n",
      " Instance 5500; Time 2.5s; loss 9.209e+04; acc 53949.0/75715.0=0.7125\n",
      " Instance 6000; Time 2.7s; loss 1.505e+05; acc 59207.0/82543.0=0.7173\n",
      " Instance 6500; Time 2.7s; loss 1.226e+05; acc 63841.0/89169.0=0.716\n",
      " Instance 7000; Time 3.1s; loss 1.072e+05; acc 68475.0/95941.0=0.7137\n",
      " Instance 7500; Time 3.1s; loss 1.628e+05; acc 73575.0/102875.0=0.7152\n",
      " Instance 8000; Time 3.0s; loss 2.273e+05; acc 78295.0/109699.0=0.7137\n",
      " Instance 8500; Time 2.9s; loss 9.26e+04; acc 82997.0/116554.0=0.7121\n",
      " Instance 9000; Time 2.3s; loss 7.545e+04; acc 87627.0/123320.0=0.7106\n",
      " Instance 9500; Time 2.4s; loss 1.618e+05; acc 93070.0/130720.0=0.712\n",
      " Instance 10000; Time 3.4s; loss 9.694e+04; acc 97417.0/137537.0=0.7083\n",
      " Instance 10500; Time 2.6s; loss 1.807e+05; acc 102280.0/143903.0=0.7108\n",
      " Instance 11000; Time 2.4s; loss 1.366e+05; acc 106924.0/150364.0=0.7111\n",
      " Instance 11500; Time 2.6s; loss 1.214e+05; acc 111643.0/156878.0=0.7117\n",
      " Instance 12000; Time 2.9s; loss 1.56e+05; acc 115884.0/163437.0=0.709\n",
      " Instance 12500; Time 2.3s; loss 1.396e+05; acc 120437.0/170329.0=0.7071\n",
      " Instance 13000; Time 2.4s; loss 1.552e+05; acc 125758.0/177550.0=0.7083\n",
      " Instance 13500; Time 2.9s; loss 1.268e+05; acc 130878.0/184229.0=0.7104\n",
      " Instance 14000; Time 2.9s; loss 1.401e+05; acc 135895.0/191269.0=0.7105\n",
      " Instance 14500; Time 2.9s; loss 1.242e+05; acc 140755.0/198070.0=0.7106\n",
      " Instance 14986; Time 3.1s; loss 9.061e+04; acc 145320.0/204566.0=0.7104\n",
      " Epoch: 1 training finished. Time: 8.2e+01s; speed: 1.8e+02st/s; total loss: 3852684.1640625\n",
      "gold_num = 189; predict_num = 454; right_num = 36\n",
      "Dev: time: 0.2s, speed 1.8e+04st/s; acc: 0.4209, p: 0.0793, r: 0.1905, f: 0.112\n",
      "gold_num = 211; predict_num = 491; right_num = 57\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Test: time: 0.2s, speed 2e+04st/s; acc: 0.471, p: 0.1161, r: 0.2701, f: 0.1624\n",
      "\"Exceed previous best f score: -1.0\n",
      "Save current best model in file: ../pretrained/myModel/myModel.1.model\n",
      "Epoch 2/5\n",
      " Learning rate is setted as: 0.013636363636363634\n",
      " Instance 500; Time 2.5s; loss 1.055e+05; acc 4592.0/6524.0=0.7039\n",
      " Instance 1000; Time 3.3s; loss 1.308e+05; acc 9026.0/13245.0=0.6815\n",
      " Instance 1500; Time 3.0s; loss 1.23e+05; acc 14131.0/20480.0=0.69\n",
      " Instance 2000; Time 3.1s; loss 1.348e+05; acc 19036.0/27246.0=0.6987\n",
      " Instance 2500; Time 2.8s; loss 8.247e+04; acc 23850.0/33879.0=0.704\n",
      " Instance 3000; Time 2.4s; loss 6.757e+04; acc 28286.0/40370.0=0.7007\n",
      " Instance 3500; Time 3.1s; loss 1.137e+05; acc 33447.0/47238.0=0.7081\n",
      " Instance 4000; Time 2.7s; loss 1.323e+05; acc 37558.0/53737.0=0.6989\n",
      " Instance 4500; Time 2.8s; loss 1.309e+05; acc 42238.0/60448.0=0.6987\n",
      " Instance 5000; Time 3.5s; loss 1.269e+05; acc 47854.0/67660.0=0.7073\n",
      " Instance 5500; Time 2.9s; loss 1.098e+05; acc 52511.0/74167.0=0.708\n",
      " Instance 6000; Time 2.7s; loss 1.146e+05; acc 56644.0/80664.0=0.7022\n",
      " Instance 6500; Time 2.5s; loss 1.31e+05; acc 61698.0/87542.0=0.7048\n",
      " Instance 7000; Time 2.7s; loss 1.126e+05; acc 66794.0/94354.0=0.7079\n",
      " Instance 7500; Time 3.1s; loss 1.045e+05; acc 71599.0/101329.0=0.7066\n",
      " Instance 8000; Time 3.0s; loss 8.9e+04; acc 76136.0/107492.0=0.7083\n",
      " Instance 8500; Time 2.8s; loss 6.844e+04; acc 80436.0/114019.0=0.7055\n",
      " Instance 9000; Time 2.9s; loss 1.003e+05; acc 85502.0/121257.0=0.7051\n",
      " Instance 9500; Time 2.9s; loss 1.068e+05; acc 90395.0/127723.0=0.7077\n",
      " Instance 10000; Time 2.7s; loss 1.319e+05; acc 94898.0/134566.0=0.7052\n",
      " Instance 10500; Time 2.5s; loss 1.412e+05; acc 100581.0/142092.0=0.7079\n",
      " Instance 11000; Time 3.2s; loss 1.139e+05; acc 105153.0/148932.0=0.706\n",
      " Instance 11500; Time 2.8s; loss 1.012e+05; acc 110067.0/155370.0=0.7084\n",
      " Instance 12000; Time 3.1s; loss 1.383e+05; acc 115173.0/162364.0=0.7094\n",
      " Instance 12500; Time 2.8s; loss 1.167e+05; acc 119903.0/169145.0=0.7089\n",
      " Instance 13000; Time 3.1s; loss 9.407e+04; acc 124458.0/175794.0=0.708\n",
      " Instance 13500; Time 2.4s; loss 1.077e+05; acc 129707.0/182596.0=0.7103\n",
      " Instance 14000; Time 2.6s; loss 1.441e+05; acc 134847.0/190122.0=0.7093\n",
      " Instance 14500; Time 3.1s; loss 1.522e+05; acc 140293.0/197560.0=0.7101\n",
      " Instance 14986; Time 3.0s; loss 1.479e+05; acc 145495.0/204566.0=0.7112\n",
      " Epoch: 2 training finished. Time: 8.6e+01s; speed: 1.7e+02st/s; total loss: 3474191.576171875\n",
      "gold_num = 189; predict_num = 0; right_num = 0\n",
      "Dev: time: 0.2s, speed 1.8e+04st/s; acc: 0.8021, p: -1.0, r: 0.0, f: -1.0\n",
      "gold_num = 211; predict_num = 0; right_num = 0\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Test: time: 0.2s, speed 1.8e+04st/s; acc: 0.7656, p: -1.0, r: 0.0, f: -1.0\n",
      "Epoch 3/5\n",
      " Learning rate is setted as: 0.013043478260869566\n",
      " Instance 500; Time 2.5s; loss 9.75e+04; acc 4868.0/7010.0=0.6944\n",
      " Instance 1000; Time 2.8s; loss 1.55e+05; acc 10164.0/14249.0=0.7133\n",
      " Instance 1500; Time 3.4s; loss 7.722e+04; acc 14960.0/21140.0=0.7077\n",
      " Instance 2000; Time 3.5s; loss 9.473e+04; acc 19655.0/27785.0=0.7074\n",
      " Instance 2500; Time 2.8s; loss 1.136e+05; acc 24815.0/34660.0=0.716\n",
      " Instance 3000; Time 2.5s; loss 8.456e+04; acc 29419.0/41233.0=0.7135\n",
      " Instance 3500; Time 3.0s; loss 7.369e+04; acc 34229.0/47892.0=0.7147\n",
      " Instance 4000; Time 2.8s; loss 9.837e+04; acc 39053.0/54670.0=0.7143\n",
      " Instance 4500; Time 3.1s; loss 8.631e+04; acc 44048.0/61720.0=0.7137\n",
      " Instance 5000; Time 2.8s; loss 9.427e+04; acc 48782.0/68640.0=0.7107\n",
      " Instance 5500; Time 2.7s; loss 5.499e+04; acc 53244.0/74619.0=0.7135\n",
      " Instance 6000; Time 2.8s; loss 9.845e+04; acc 57733.0/81508.0=0.7083\n",
      " Instance 6500; Time 3.1s; loss 1.022e+05; acc 62928.0/88460.0=0.7114\n",
      " Instance 7000; Time 3.6s; loss 9.441e+04; acc 67766.0/95468.0=0.7098\n",
      " Instance 7500; Time 2.7s; loss 1.215e+05; acc 72534.0/102358.0=0.7086\n",
      " Instance 8000; Time 2.7s; loss 8.966e+04; acc 77584.0/109128.0=0.7109\n",
      " Instance 8500; Time 3.1s; loss 8.741e+04; acc 82280.0/116017.0=0.7092\n",
      " Instance 9000; Time 3.0s; loss 9.826e+04; acc 86850.0/122738.0=0.7076\n",
      " Instance 9500; Time 3.3s; loss 1.268e+05; acc 91653.0/129472.0=0.7079\n",
      " Instance 10000; Time 3.1s; loss 6.787e+04; acc 96573.0/136423.0=0.7079\n",
      " Instance 10500; Time 2.6s; loss 1.08e+05; acc 101791.0/143686.0=0.7084\n",
      " Instance 11000; Time 3.0s; loss 8.338e+04; acc 107225.0/150775.0=0.7112\n",
      " Instance 11500; Time 3.6s; loss 8.802e+04; acc 112180.0/157747.0=0.7111\n",
      " Instance 12000; Time 3.7s; loss 7.765e+04; acc 117258.0/165061.0=0.7104\n",
      " Instance 12500; Time 2.6s; loss 8.51e+04; acc 121931.0/171371.0=0.7115\n",
      " Instance 13000; Time 2.5s; loss 8.356e+04; acc 127167.0/178500.0=0.7124\n",
      " Instance 13500; Time 3.1s; loss 9.018e+04; acc 131766.0/185129.0=0.7118\n",
      " Instance 14000; Time 3.0s; loss 7.307e+04; acc 136116.0/191685.0=0.7101\n",
      " Instance 14500; Time 2.8s; loss 1.199e+05; acc 140835.0/198131.0=0.7108\n",
      " Instance 14986; Time 3.1s; loss 7.901e+04; acc 145682.0/204566.0=0.7122\n",
      " Epoch: 3 training finished. Time: 8.9e+01s; speed: 1.7e+02st/s; total loss: 2804598.34765625\n",
      "gold_num = 189; predict_num = 0; right_num = 0\n",
      "Dev: time: 0.14s, speed 2.4e+04st/s; acc: 0.8021, p: -1.0, r: 0.0, f: -1.0\n",
      "gold_num = 211; predict_num = 0; right_num = 0\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Test: time: 0.14s, speed 1.6e+04st/s; acc: 0.7656, p: -1.0, r: 0.0, f: -1.0\n",
      "Epoch 4/5\n",
      " Learning rate is setted as: 0.0125\n",
      " Instance 500; Time 2.6s; loss 8.489e+04; acc 4654.0/6923.0=0.6723\n",
      " Instance 1000; Time 3.0s; loss 1.037e+05; acc 9298.0/13415.0=0.6931\n",
      " Instance 1500; Time 2.8s; loss 8.197e+04; acc 14369.0/20136.0=0.7136\n",
      " Instance 2000; Time 2.7s; loss 1.038e+05; acc 19372.0/27347.0=0.7084\n",
      " Instance 2500; Time 3.0s; loss 9.661e+04; acc 24180.0/34529.0=0.7003\n",
      " Instance 3000; Time 3.2s; loss 9.376e+04; acc 29222.0/41208.0=0.7091\n",
      " Instance 3500; Time 3.4s; loss 1.037e+05; acc 33861.0/48046.0=0.7048\n",
      " Instance 4000; Time 2.4s; loss 6.975e+04; acc 38482.0/54663.0=0.704\n",
      " Instance 4500; Time 2.9s; loss 1.14e+05; acc 43476.0/61636.0=0.7054\n",
      " Instance 5000; Time 3.4s; loss 9.642e+04; acc 48700.0/68542.0=0.7105\n",
      " Instance 5500; Time 2.9s; loss 6.713e+04; acc 53498.0/75534.0=0.7083\n",
      " Instance 6000; Time 2.9s; loss 9.306e+04; acc 58545.0/82203.0=0.7122\n",
      " Instance 6500; Time 2.9s; loss 8.407e+04; acc 63365.0/89244.0=0.71\n",
      " Instance 7000; Time 3.5s; loss 9.699e+04; acc 67943.0/95607.0=0.7106\n",
      " Instance 7500; Time 3.1s; loss 5.445e+04; acc 72656.0/102164.0=0.7112\n",
      " Instance 8000; Time 3.2s; loss 9.093e+04; acc 77794.0/109221.0=0.7123\n",
      " Instance 8500; Time 3.2s; loss 7.298e+04; acc 82735.0/115949.0=0.7135\n",
      " Instance 9000; Time 2.9s; loss 1.339e+05; acc 87520.0/123166.0=0.7106\n",
      " Instance 9500; Time 3.3s; loss 7.233e+04; acc 92000.0/129792.0=0.7088\n",
      " Instance 10000; Time 3.1s; loss 6.5e+04; acc 97185.0/136561.0=0.7117\n",
      " Instance 10500; Time 3.3s; loss 9.211e+04; acc 102030.0/143448.0=0.7113\n",
      " Instance 11000; Time 2.9s; loss 8.701e+04; acc 106685.0/150256.0=0.71\n",
      " Instance 11500; Time 2.7s; loss 6.614e+04; acc 111077.0/156782.0=0.7085\n",
      " Instance 12000; Time 3.0s; loss 9.194e+04; acc 116304.0/163697.0=0.7105\n",
      " Instance 12500; Time 3.5s; loss 8.996e+04; acc 120922.0/170659.0=0.7086\n",
      " Instance 13000; Time 3.6s; loss 7.384e+04; acc 125425.0/177118.0=0.7081\n",
      " Instance 13500; Time 3.0s; loss 6.261e+04; acc 130747.0/184230.0=0.7097\n",
      " Instance 14000; Time 2.6s; loss 5.697e+04; acc 135654.0/191031.0=0.7101\n",
      " Instance 14500; Time 2.8s; loss 6.225e+04; acc 140142.0/197771.0=0.7086\n",
      " Instance 14986; Time 3.4s; loss 7.234e+04; acc 144971.0/204566.0=0.7087\n",
      " Epoch: 4 training finished. Time: 9.1e+01s; speed: 1.6e+02st/s; total loss: 2534591.9462890625\n",
      "gold_num = 189; predict_num = 0; right_num = 0\n",
      "Dev: time: 0.14s, speed 2.5e+04st/s; acc: 0.8021, p: -1.0, r: 0.0, f: -1.0\n",
      "gold_num = 211; predict_num = 0; right_num = 0\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Test: time: 0.14s, speed 1.9e+04st/s; acc: 0.7656, p: -1.0, r: 0.0, f: -1.0\n"
     ]
    }
   ],
   "source": [
    "train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to decode a new input from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model weights from file ../pretrained/myModel/myModel.1.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/myModel/myModel.xpt'\n",
    "path2model = '../pretrained/myModel/myModel.1.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.load_model_dir = path2model\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2write = out_folder + os.path.basename(os.path.splitext(file_name)[0]) + '.out'\n",
    "# open and return the text of the file\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.091 s\n",
      "Decoding speed: 319.2 st/s\n",
      "[['B-ORG', 'O', 'B-PER', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O']]\n",
      "['B-ORG O B-PER O O B-PER O O O B-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O O O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER O B-PER O B-PER O O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O O O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O O O B-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER O B-PER O O O B-PER I-PER O B-PER I-PER O B-PER O O B-PER O B-PER I-PER O B-PER O O B-PER I-PER O\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training informations of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
