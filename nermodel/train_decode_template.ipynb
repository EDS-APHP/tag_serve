{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import train, data_initialization, build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model on conll2003 shared task training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2train = '../conll2003/train.conll2003'\n",
    "path2dev = '../conll2003/dev.conll2003'\n",
    "path2test = '../conll2003/test.conll2003'\n",
    "path2model = '../pretrained/myModel/myModel'\n",
    "modelDir = os.path.join(*path2model.split('/')[:-1])\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "path2emb = '../pretrained/glove.6B.50d.txt'\n",
    "confdict = {# IO\n",
    "            'train_dir':path2train,\n",
    "            'dev_dir':path2dev,\n",
    "            'test_dir':path2test,\n",
    "            'model_dir':path2model,\n",
    "            # Embeddings\n",
    "            'word_emb_dir':path2emb,\n",
    "            'char_emb_dir':None,\n",
    "            'word_emb_dim':50,\n",
    "            'char_emb_dim':30,\n",
    "            # Network\n",
    "            'use_crf':True,\n",
    "            'use_char':True,\n",
    "            'use_feats': False,\n",
    "            'word_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            'char_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            # HP\n",
    "            'HP_cnn_layer':4 ,\n",
    "            'HP_char_hidden_dim':50,\n",
    "            'HP_hidden_dim':200,\n",
    "            'HP_dropout':0.5,\n",
    "            'HP_lstm_layer':1,\n",
    "            'HP_bilstm':True,\n",
    "            'HP_lr':0.015,\n",
    "            # training\n",
    "            'optimizer':'SGD',\n",
    "            'batch_size':10,\n",
    "            'iteration':5\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained word embedding, norm False, dir: ../pretrained/glove.6B.50d.txt\n",
      "Embedding: \n",
      " pretrain words: 400000, perfect_match: 14618, case_match: 11722, oov: 3951\n",
      "Training model...\n",
      "****************************************\n",
      "----------Data summary:----------\n",
      "\n",
      " HP_gpu: False\n",
      " MAX_SENTENCE_LENTGH: 1000\n",
      " number_normalized: False\n",
      " word_alphabet: 30292\n",
      " char_alphabet_size: 87\n",
      " label_alphabet_size: 10\n",
      " load_model_dir: None\n",
      "\n",
      "\n",
      "I/O:\n",
      " tagScheme: BIO\n",
      " train_dir: ../conll2003/train.conll2003\n",
      " dev_dir: ../conll2003/dev.conll2003\n",
      " test_dir: ../conll2003/test.conll2003\n",
      " raw_dir: None\n",
      " dset_dir: None\n",
      " word_emb_dir: ../pretrained/glove.6B.50d.txt\n",
      " char_emb_dir: None\n",
      " feature_emb_dirs: []\n",
      "\n",
      "\n",
      "Network:\n",
      " word_feature_extractor: LSTM\n",
      " use_char: True\n",
      " char_feature_extractor: LSTM\n",
      " use_crf: True\n",
      "\n",
      "\n",
      "Network Hyperparameters:\n",
      " word_emb_dim: 50\n",
      " char_emb_dim: 30\n",
      " feature_emb_dims: []\n",
      " HP_char_hidden_dim: 50\n",
      " HP_hidden_dim: 200\n",
      " HP_lstm_layer: 1\n",
      " HP_bilstm: True\n",
      " HP_cnn_layer: 4\n",
      " HP_dropout: 0.5\n",
      "\n",
      "\n",
      "Training Hyperparameters:\n",
      " average_batch_loss: False\n",
      " optimizer: SGD\n",
      " iteration: 5\n",
      " batch_size: 10\n",
      " HP_lr: 0.015\n",
      " HP_lr_decayr: 0.05\n",
      " HP_clip: None\n",
      " HP_momentum: 0\n",
      " HP_l2: 1e-08\n",
      "****************************************\n",
      "\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  LSTM\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "build char sequence feature extractor: LSTM ...\n",
      "build CRF...\n",
      "Epoch 0/5\n",
      " Learning rate is setted as: 0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/Documents/projets/tag_serve/nermodel/ner_model.py:378: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  sample_loss += loss.data[0]\n",
      "/home/matthieu/Documents/projets/tag_serve/nermodel/ner_model.py:379: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  total_loss += loss.data[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Instance 500; Time 4.1s; loss 5.33e+03; acc 5799.0/7183.0=0.8073\n",
      " Instance 1000; Time 3.8s; loss 3.257e+03; acc 11769.0/14167.0=0.8307\n",
      " Instance 1500; Time 4.0s; loss 2.762e+03; acc 17739.0/21084.0=0.8413\n",
      " Instance 2000; Time 4.8s; loss 2.796e+03; acc 23416.0/27742.0=0.8441\n",
      " Instance 2500; Time 5.1s; loss 2.186e+03; acc 29392.0/34521.0=0.8514\n",
      " Instance 3000; Time 5.5s; loss 1.906e+03; acc 35498.0/41362.0=0.8582\n",
      " Instance 3500; Time 5.5s; loss 1.939e+03; acc 41546.0/48120.0=0.8634\n",
      " Instance 4000; Time 4.0s; loss 1.608e+03; acc 47637.0/54842.0=0.8686\n",
      " Instance 4500; Time 3.7s; loss 1.406e+03; acc 53783.0/61526.0=0.8742\n",
      " Instance 5000; Time 4.7s; loss 1.358e+03; acc 60070.0/68334.0=0.8791\n",
      " Instance 5500; Time 5.2s; loss 1.29e+03; acc 66208.0/74994.0=0.8828\n",
      " Instance 6000; Time 4.3s; loss 1.128e+03; acc 72203.0/81444.0=0.8865\n",
      " Instance 6500; Time 4.5s; loss 1.139e+03; acc 79018.0/88732.0=0.8905\n",
      " Instance 7000; Time 4.3s; loss 1.061e+03; acc 85678.0/95821.0=0.8941\n",
      " Instance 7500; Time 5.5s; loss 1.037e+03; acc 92012.0/102577.0=0.897\n",
      " Instance 8000; Time 5.3s; loss 927.4; acc 98300.0/109257.0=0.8997\n",
      " Instance 8500; Time 4.1s; loss 919.6; acc 104734.0/116031.0=0.9026\n",
      " Instance 9000; Time 4.1s; loss 863.0; acc 111224.0/122890.0=0.9051\n",
      " Instance 9500; Time 4.7s; loss 932.7; acc 117936.0/129990.0=0.9073\n",
      " Instance 10000; Time 5.6s; loss 985.4; acc 124757.0/137220.0=0.9092\n",
      " Instance 10500; Time 5.6s; loss 872.1; acc 131246.0/144065.0=0.911\n",
      " Instance 11000; Time 4.0s; loss 856.2; acc 137479.0/150661.0=0.9125\n",
      " Instance 11500; Time 5.3s; loss 870.2; acc 144310.0/157849.0=0.9142\n",
      " Instance 12000; Time 5.2s; loss 853.3; acc 150785.0/164669.0=0.9157\n",
      " Instance 12500; Time 5.3s; loss 780.7; acc 157013.0/171204.0=0.9171\n",
      " Instance 13000; Time 5.6s; loss 861.6; acc 163314.0/177845.0=0.9183\n",
      " Instance 13500; Time 5.4s; loss 745.7; acc 169587.0/184454.0=0.9194\n",
      " Instance 14000; Time 5.6s; loss 802.0; acc 176235.0/191414.0=0.9207\n",
      " Instance 14500; Time 5.5s; loss 800.2; acc 182822.0/198308.0=0.9219\n",
      " Instance 14986; Time 5.1s; loss 748.4; acc 188737.0/204566.0=0.9226\n",
      " Epoch: 0 training finished. Time: 1.5e+02s; speed: 1e+02st/s; total loss: 43022.23046875\n",
      "gold_num = 5913; predict_num = 5908; right_num = 5085\n",
      "Dev: time: 1e+01s, speed 3.4e+02st/s; acc: 0.9725, p: 0.8607, r: 0.86, f: 0.8603\n",
      "gold_num = 5596; predict_num = 5632; right_num = 4687\n",
      "Test: time: 1e+01s, speed 3.9e+02st/s; acc: 0.9653, p: 0.8322, r: 0.8376, f: 0.8349\n",
      "\"Exceed previous best f score: -10\n",
      "Save current best model in file: ../pretrained/myModel/myModel.0.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 1/5\n",
      " Learning rate is setted as: 0.014285714285714285\n",
      " Instance 500; Time 5.3s; loss 813.4; acc 6488.0/6815.0=0.952\n",
      " Instance 1000; Time 5.7s; loss 808.3; acc 13297.0/13935.0=0.9542\n",
      " Instance 1500; Time 5.6s; loss 719.0; acc 19739.0/20681.0=0.9545\n",
      " Instance 2000; Time 4.4s; loss 669.5; acc 26410.0/27632.0=0.9558\n",
      " Instance 2500; Time 4.0s; loss 784.5; acc 32815.0/34386.0=0.9543\n",
      " Instance 3000; Time 5.5s; loss 738.3; acc 39271.0/41151.0=0.9543\n",
      " Instance 3500; Time 5.3s; loss 617.9; acc 45427.0/47565.0=0.9551\n",
      " Instance 4000; Time 5.3s; loss 713.2; acc 51578.0/54025.0=0.9547\n",
      " Instance 4500; Time 5.5s; loss 771.1; acc 58063.0/60821.0=0.9547\n",
      " Instance 5000; Time 4.4s; loss 668.8; acc 64553.0/67560.0=0.9555\n",
      " Instance 5500; Time 5.5s; loss 756.5; acc 71647.0/74974.0=0.9556\n",
      " Instance 6000; Time 5.4s; loss 690.1; acc 78137.0/81732.0=0.956\n",
      " Instance 6500; Time 5.5s; loss 662.1; acc 84747.0/88646.0=0.956\n",
      " Instance 7000; Time 5.4s; loss 748.0; acc 91349.0/95583.0=0.9557\n",
      " Instance 7500; Time 5.5s; loss 620.3; acc 98062.0/102534.0=0.9564\n",
      " Instance 8000; Time 5.5s; loss 715.7; acc 104438.0/109210.0=0.9563\n",
      " Instance 8500; Time 4.3s; loss 707.7; acc 111039.0/116084.0=0.9565\n",
      " Instance 9000; Time 5.2s; loss 733.5; acc 117535.0/122880.0=0.9565\n",
      " Instance 9500; Time 5.3s; loss 756.4; acc 124300.0/129964.0=0.9564\n",
      " Instance 10000; Time 5.4s; loss 634.0; acc 130830.0/136779.0=0.9565\n",
      " Instance 10500; Time 5.4s; loss 655.4; acc 137376.0/143624.0=0.9565\n",
      " Instance 11000; Time 5.4s; loss 631.0; acc 144095.0/150624.0=0.9567\n",
      " Instance 11500; Time 5.3s; loss 634.2; acc 150331.0/157123.0=0.9568\n",
      " Instance 12000; Time 5.4s; loss 679.6; acc 156954.0/164020.0=0.9569\n",
      " Instance 12500; Time 5.5s; loss 619.0; acc 163363.0/170689.0=0.9571\n",
      " Instance 13000; Time 5.2s; loss 703.6; acc 169418.0/177080.0=0.9567\n",
      " Instance 13500; Time 4.6s; loss 646.4; acc 176100.0/184026.0=0.9569\n",
      " Instance 14000; Time 5.4s; loss 628.2; acc 182929.0/191132.0=0.9571\n",
      " Instance 14500; Time 5.3s; loss 652.6; acc 189290.0/197747.0=0.9572\n",
      " Instance 14986; Time 4.4s; loss 641.1; acc 195838.0/204566.0=0.9573\n",
      " Epoch: 1 training finished. Time: 1.6e+02s; speed: 9.6e+01st/s; total loss: 20819.830078125\n",
      "gold_num = 5913; predict_num = 5913; right_num = 5237\n",
      "Dev: time: 1e+01s, speed 3.4e+02st/s; acc: 0.9771, p: 0.8857, r: 0.8857, f: 0.8857\n",
      "gold_num = 5596; predict_num = 5633; right_num = 4843\n",
      "Test: time: 1e+01s, speed 3.9e+02st/s; acc: 0.9699, p: 0.8598, r: 0.8654, f: 0.8626\n",
      "\"Exceed previous best f score: 0.8603333051349293\n",
      "Save current best model in file: ../pretrained/myModel/myModel.1.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 2/5\n",
      " Learning rate is setted as: 0.013636363636363634\n",
      " Instance 500; Time 5.3s; loss 597.0; acc 6698.0/6948.0=0.964\n",
      " Instance 1000; Time 5.6s; loss 625.6; acc 13498.0/14035.0=0.9617\n",
      " Instance 1500; Time 5.7s; loss 511.4; acc 19936.0/20688.0=0.9637\n",
      " Instance 2000; Time 5.4s; loss 624.2; acc 26797.0/27797.0=0.964\n",
      " Instance 2500; Time 5.4s; loss 576.7; acc 33213.0/34486.0=0.9631\n",
      " Instance 3000; Time 5.2s; loss 590.0; acc 39328.0/40832.0=0.9632\n",
      " Instance 3500; Time 5.4s; loss 506.8; acc 45937.0/47661.0=0.9638\n",
      " Instance 4000; Time 5.3s; loss 544.9; acc 52410.0/54367.0=0.964\n",
      " Instance 4500; Time 5.2s; loss 571.2; acc 58583.0/60785.0=0.9638\n",
      " Instance 5000; Time 5.3s; loss 620.9; acc 65017.0/67486.0=0.9634\n",
      " Instance 5500; Time 5.7s; loss 603.3; acc 71912.0/74631.0=0.9636\n",
      " Instance 6000; Time 4.5s; loss 675.4; acc 78517.0/81526.0=0.9631\n",
      " Instance 6500; Time 5.3s; loss 608.7; acc 85185.0/88437.0=0.9632\n",
      " Instance 7000; Time 5.6s; loss 611.0; acc 91975.0/95464.0=0.9635\n",
      " Instance 7500; Time 5.4s; loss 653.1; acc 98542.0/102330.0=0.963\n",
      " Instance 8000; Time 5.5s; loss 577.2; acc 105188.0/109196.0=0.9633\n",
      " Instance 8500; Time 4.7s; loss 592.6; acc 111877.0/116139.0=0.9633\n",
      " Instance 9000; Time 4.3s; loss 559.0; acc 118032.0/122542.0=0.9632\n",
      " Instance 9500; Time 5.6s; loss 595.1; acc 125117.0/129874.0=0.9634\n",
      " Instance 10000; Time 5.4s; loss 457.7; acc 131666.0/136613.0=0.9638\n",
      " Instance 10500; Time 5.3s; loss 615.1; acc 138186.0/143403.0=0.9636\n",
      " Instance 11000; Time 5.2s; loss 585.1; acc 144412.0/149863.0=0.9636\n",
      " Instance 11500; Time 5.4s; loss 608.8; acc 151215.0/156918.0=0.9637\n",
      " Instance 12000; Time 5.5s; loss 617.4; acc 158017.0/163974.0=0.9637\n",
      " Instance 12500; Time 5.5s; loss 678.1; acc 164749.0/171009.0=0.9634\n",
      " Instance 13000; Time 5.3s; loss 572.4; acc 170948.0/177467.0=0.9633\n",
      " Instance 13500; Time 5.7s; loss 552.5; acc 177847.0/184603.0=0.9634\n",
      " Instance 14000; Time 5.7s; loss 596.3; acc 184443.0/191457.0=0.9634\n",
      " Instance 14500; Time 5.8s; loss 575.0; acc 191045.0/198288.0=0.9635\n",
      " Instance 14986; Time 5.9s; loss 418.3; acc 197130.0/204566.0=0.9636\n",
      " Epoch: 2 training finished. Time: 1.6e+02s; speed: 9.3e+01st/s; total loss: 17520.626953125\n",
      "gold_num = 5913; predict_num = 5961; right_num = 5354\n",
      "Dev: time: 1.2e+01s, speed 2.8e+02st/s; acc: 0.9803, p: 0.8982, r: 0.9055, f: 0.9018\n",
      "gold_num = 5596; predict_num = 5705; right_num = 4904\n",
      "Test: time: 1.2e+01s, speed 3.6e+02st/s; acc: 0.9713, p: 0.8596, r: 0.8763, f: 0.8679\n",
      "\"Exceed previous best f score: 0.8856756299678674\n",
      "Save current best model in file: ../pretrained/myModel/myModel.2.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 3/5\n",
      " Learning rate is setted as: 0.013043478260869566\n",
      " Instance 500; Time 6.4s; loss 527.4; acc 7062.0/7304.0=0.9669\n",
      " Instance 1000; Time 4.7s; loss 546.9; acc 13586.0/14047.0=0.9672\n",
      " Instance 1500; Time 4.4s; loss 586.9; acc 20548.0/21242.0=0.9673\n",
      " Instance 2000; Time 3.9s; loss 442.4; acc 27175.0/28061.0=0.9684\n",
      " Instance 2500; Time 4.2s; loss 514.7; acc 33733.0/34833.0=0.9684\n",
      " Instance 3000; Time 4.5s; loss 505.9; acc 40515.0/41829.0=0.9686\n",
      " Instance 3500; Time 3.9s; loss 460.8; acc 46820.0/48327.0=0.9688\n",
      " Instance 4000; Time 3.9s; loss 532.8; acc 53052.0/54793.0=0.9682\n",
      " Instance 4500; Time 3.8s; loss 493.8; acc 59231.0/61174.0=0.9682\n",
      " Instance 5000; Time 4.2s; loss 577.6; acc 65832.0/67994.0=0.9682\n",
      " Instance 5500; Time 4.7s; loss 502.6; acc 72554.0/74945.0=0.9681\n",
      " Instance 6000; Time 5.4s; loss 432.0; acc 79340.0/81909.0=0.9686\n",
      " Instance 6500; Time 5.5s; loss 639.8; acc 85790.0/88622.0=0.968\n",
      " Instance 7000; Time 5.4s; loss 458.6; acc 92275.0/95293.0=0.9683\n",
      " Instance 7500; Time 5.5s; loss 566.5; acc 98862.0/102119.0=0.9681\n",
      " Instance 8000; Time 4.3s; loss 529.3; acc 105890.0/109358.0=0.9683\n",
      " Instance 8500; Time 4.0s; loss 526.3; acc 112554.0/116252.0=0.9682\n",
      " Instance 9000; Time 4.1s; loss 542.1; acc 119518.0/123443.0=0.9682\n",
      " Instance 9500; Time 4.0s; loss 431.2; acc 126431.0/130532.0=0.9686\n",
      " Instance 10000; Time 3.8s; loss 514.1; acc 132879.0/137204.0=0.9685\n",
      " Instance 10500; Time 3.9s; loss 584.8; acc 139266.0/143820.0=0.9683\n",
      " Instance 11000; Time 4.0s; loss 474.9; acc 145725.0/150497.0=0.9683\n",
      " Instance 11500; Time 4.0s; loss 534.7; acc 152438.0/157422.0=0.9683\n",
      " Instance 12000; Time 3.8s; loss 545.4; acc 158623.0/163845.0=0.9681\n",
      " Instance 12500; Time 3.9s; loss 536.6; acc 165348.0/170766.0=0.9683\n",
      " Instance 13000; Time 4.0s; loss 517.7; acc 171694.0/177299.0=0.9684\n",
      " Instance 13500; Time 3.9s; loss 529.4; acc 178165.0/183988.0=0.9684\n",
      " Instance 14000; Time 5.4s; loss 550.8; acc 184855.0/190926.0=0.9682\n",
      " Instance 14500; Time 5.1s; loss 514.0; acc 191786.0/198078.0=0.9682\n",
      " Instance 14986; Time 4.7s; loss 525.2; acc 198058.0/204566.0=0.9682\n",
      " Epoch: 3 training finished. Time: 1.3e+02s; speed: 1.1e+02st/s; total loss: 15645.3056640625\n",
      "gold_num = 5913; predict_num = 5925; right_num = 5349\n",
      "Dev: time: 1.1e+01s, speed 3.2e+02st/s; acc: 0.9808, p: 0.9028, r: 0.9046, f: 0.9037\n",
      "gold_num = 5596; predict_num = 5648; right_num = 4886\n",
      "Test: time: 1.1e+01s, speed 3.9e+02st/s; acc: 0.9716, p: 0.8651, r: 0.8731, f: 0.8691\n",
      "\"Exceed previous best f score: 0.9018022570321711\n",
      "Save current best model in file: ../pretrained/myModel/myModel.3.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 4/5\n",
      " Learning rate is setted as: 0.0125\n",
      " Instance 500; Time 5.0s; loss 489.3; acc 7009.0/7191.0=0.9747\n",
      " Instance 1000; Time 4.4s; loss 479.5; acc 13534.0/13928.0=0.9717\n",
      " Instance 1500; Time 4.4s; loss 486.1; acc 20161.0/20739.0=0.9721\n",
      " Instance 2000; Time 4.0s; loss 478.5; acc 26172.0/26941.0=0.9715\n",
      " Instance 2500; Time 4.5s; loss 502.5; acc 32939.0/33920.0=0.9711\n",
      " Instance 3000; Time 4.7s; loss 457.9; acc 39488.0/40701.0=0.9702\n",
      " Instance 3500; Time 4.5s; loss 485.1; acc 46405.0/47826.0=0.9703\n",
      " Instance 4000; Time 5.5s; loss 453.4; acc 53056.0/54671.0=0.9705\n",
      " Instance 4500; Time 5.4s; loss 511.7; acc 59710.0/61534.0=0.9704\n",
      " Instance 5000; Time 5.4s; loss 467.1; acc 66396.0/68415.0=0.9705\n",
      " Instance 5500; Time 4.7s; loss 525.6; acc 72800.0/75040.0=0.9701\n",
      " Instance 6000; Time 4.5s; loss 486.5; acc 79340.0/81785.0=0.9701\n",
      " Instance 6500; Time 4.6s; loss 432.3; acc 85661.0/88284.0=0.9703\n",
      " Instance 7000; Time 5.4s; loss 460.5; acc 92033.0/94857.0=0.9702\n",
      " Instance 7500; Time 5.7s; loss 500.4; acc 98865.0/101875.0=0.9705\n",
      " Instance 8000; Time 5.7s; loss 422.7; acc 105633.0/108815.0=0.9708\n",
      " Instance 8500; Time 5.3s; loss 447.2; acc 112099.0/115480.0=0.9707\n",
      " Instance 9000; Time 5.7s; loss 543.4; acc 118955.0/122582.0=0.9704\n",
      " Instance 9500; Time 4.2s; loss 371.1; acc 125503.0/129295.0=0.9707\n",
      " Instance 10000; Time 3.9s; loss 504.6; acc 131904.0/135915.0=0.9705\n",
      " Instance 10500; Time 4.1s; loss 429.9; acc 138600.0/142796.0=0.9706\n",
      " Instance 11000; Time 4.6s; loss 464.3; acc 145438.0/149821.0=0.9707\n",
      " Instance 11500; Time 4.1s; loss 459.9; acc 151965.0/156516.0=0.9709\n",
      " Instance 12000; Time 5.9s; loss 542.8; acc 158830.0/163629.0=0.9707\n",
      " Instance 12500; Time 5.2s; loss 512.2; acc 166158.0/171164.0=0.9708\n",
      " Instance 13000; Time 4.1s; loss 521.7; acc 172621.0/177825.0=0.9707\n",
      " Instance 13500; Time 4.0s; loss 506.9; acc 179225.0/184616.0=0.9708\n",
      " Instance 14000; Time 3.9s; loss 431.1; acc 185760.0/191311.0=0.971\n",
      " Instance 14500; Time 4.1s; loss 386.6; acc 192501.0/198226.0=0.9711\n",
      " Instance 14986; Time 6.2s; loss 434.9; acc 198663.0/204566.0=0.9711\n",
      " Epoch: 4 training finished. Time: 1.4e+02s; speed: 1e+02st/s; total loss: 14195.5439453125\n",
      "gold_num = 5913; predict_num = 5913; right_num = 5389\n",
      "Dev: time: 1.1e+01s, speed 3.2e+02st/s; acc: 0.9821, p: 0.9114, r: 0.9114, f: 0.9114\n",
      "gold_num = 5596; predict_num = 5642; right_num = 4913\n",
      "Test: time: 1.1e+01s, speed 3.7e+02st/s; acc: 0.9727, p: 0.8708, r: 0.8779, f: 0.8744\n",
      "\"Exceed previous best f score: 0.9036999493157628\n",
      "Save current best model in file: ../pretrained/myModel/myModel.4.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Training done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9113817013360392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization of data object and training (equivalent to main.myTrain(confdict))\n",
    "data = Data()\n",
    "data.read_config(confdict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data_initialization(data)\n",
    "data.generate_instance('train')\n",
    "data.generate_instance('dev')\n",
    "data.generate_instance('test')\n",
    "data.build_pretrain_emb()\n",
    "train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to decode a new input from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model weights from file ../pretrained/myModel/myModel.4.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  LSTM\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "build char sequence feature extractor: LSTM ...\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/myModel/myModel.xpt'\n",
    "path2model = '../pretrained/myModel/myModel.4.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.load_model_dir = path2model\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/inputs/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2write = out_folder + os.path.basename(os.path.splitext(file_name)[0]) + '.out'\n",
    "# open and return the text of the file\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.16 s\n",
      "Decoding speed: 173.6 st/s\n",
      "[['B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-MISC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], ['O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-MISC', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-LOC', 'O'], ['O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O'], ['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O'], ['O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-MISC', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'I-ORG']]\n",
      "['B-LOC O B-MISC O O O O O O O O B-MISC I-MISC O B-MISC O B-ORG O O O O O O O O O O O O O O O O B-LOC O B-LOC I-LOC O O O O O O O O O O\\n', 'O O O O O O O B-LOC O O O B-LOC I-LOC O O B-MISC I-MISC O O B-LOC I-LOC O O O O B-LOC O O B-LOC I-LOC O\\n', 'O O O O B-MISC I-MISC O B-LOC I-LOC O O O O O B-LOC O B-LOC O B-MISC O O\\n', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O O O O O\\n', 'O O O O O B-LOC O B-LOC O B-ORG O B-ORG O B-ORG O B-LOC O\\n', 'O O B-MISC I-MISC O O O O O B-LOC O O O O B-MISC O O B-MISC O O\\n', 'B-LOC O O O O O B-MISC O O O O O O O B-ORG I-ORG O O O O O O B-LOC O B-LOC O\\n', 'B-LOC O O O O B-MISC O O O B-ORG I-ORG I-ORG O O O O O B-MISC I-MISC I-MISC I-MISC O O O O O O\\n', 'O O B-MISC O B-MISC O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O\\n', 'B-LOC O B-LOC O O O O O O O O O O B-ORG I-ORG O\\n', 'O O O O O O O O O O B-MISC I-MISC O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O B-MISC O O O O O O O O O O O\\n', 'O O O O B-MISC O O O O O B-MISC I-MISC I-MISC O\\n', 'O O B-MISC I-MISC O O O O O B-LOC O\\n', 'O O O O O B-LOC O B-LOC O O O O O O O O O O O O B-MISC I-MISC I-MISC O O O\\n', 'B-LOC O O O O O B-MISC I-MISC O O O O O O O O O O O O O B-MISC I-MISC O B-MISC I-MISC I-MISC O O O O O O O B-MISC O O O O\\n', 'O O O O O O B-LOC I-LOC O O O O O O O O O O B-MISC I-MISC O\\n', 'O B-LOC I-LOC O O O B-PER I-PER I-PER O O O O O O O O O\\n', 'B-LOC O O O O O O O O O O O O O O O O O O O O B-LOC O\\n', 'B-LOC O O O O O O O O O O O O O O\\n', 'O O B-LOC O O O O O B-MISC I-MISC I-MISC I-MISC O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O O O O O O O O O O\\n', 'O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O\\n', 'O O O O O O O O B-ORG I-ORG O O B-MISC O\\n', 'O O O O O O O O O O B-MISC O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O B-ORG I-ORG I-ORG O B-LOC O O O B-ORG I-ORG\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training informations of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
