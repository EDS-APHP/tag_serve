{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import train, data_initialization, build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model on conll2003 shared task training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2train = '../conll2003/train.conll2003'\n",
    "path2dev = '../conll2003/dev.conll2003'\n",
    "path2test = '../conll2003/test.conll2003'\n",
    "path2model = '../pretrained/myModel/myModel'\n",
    "modelDir = os.path.join(*path2model.split('/')[:-1])\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "path2emb = '../pretrained/glove.6B.50d.txt'\n",
    "confdict = {# IO\n",
    "            'train_dir':path2train,\n",
    "            'dev_dir':path2dev,\n",
    "            'test_dir':path2test,\n",
    "            'model_dir':path2model,\n",
    "            # Embeddings\n",
    "            'word_emb_dir':path2emb,\n",
    "            'char_emb_dir':None,\n",
    "            'word_emb_dim':50,\n",
    "            'char_emb_dim':30,\n",
    "            # Network\n",
    "            'use_crf':True,\n",
    "            'use_char':True,\n",
    "            'use_feats': False,\n",
    "            'word_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            'char_feature_extractor':'CNN', # choose CNN/LSTM/GRU\n",
    "            # HP\n",
    "            'HP_cnn_layer':4 ,\n",
    "            'HP_char_hidden_dim':50,\n",
    "            'HP_hidden_dim':200,\n",
    "            'HP_dropout':0.5,\n",
    "            'HP_lstm_layer':1,\n",
    "            'HP_bilstm':True,\n",
    "            'HP_lr':0.015,\n",
    "            # training\n",
    "            'optimizer':'SGD',\n",
    "            'batch_size':10,\n",
    "            'iteration':5\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained word embedding, norm False, dir: ../pretrained/glove.6B.50d.txt\n",
      "Embedding: \n",
      " pretrain words: 400000, perfect_match: 14618, case_match: 11722, oov: 3951\n"
     ]
    }
   ],
   "source": [
    "# initialization of data object and training (equivalent to main.myTrain(confdict))\n",
    "\n",
    "data = Data()\n",
    "data.read_config(confdict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data_initialization(data)\n",
    "data.generate_instance('train')\n",
    "data.generate_instance('dev')\n",
    "data.generate_instance('test')\n",
    "data.build_pretrain_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "****************************************\n",
      "----------Data summary:----------\n",
      "\n",
      " HP_gpu: False\n",
      " MAX_SENTENCE_LENTGH: 1000\n",
      " number_normalized: False\n",
      " word_alphabet: 30292\n",
      " char_alphabet_size: 87\n",
      " label_alphabet_size: 10\n",
      " load_model_dir: None\n",
      "\n",
      "\n",
      "I/O:\n",
      " tagScheme: BIO\n",
      " train_dir: ../conll2003/train.conll2003\n",
      " dev_dir: ../conll2003/dev.conll2003\n",
      " test_dir: ../conll2003/test.conll2003\n",
      " raw_dir: None\n",
      " dset_dir: None\n",
      " word_emb_dir: ../pretrained/glove.6B.50d.txt\n",
      " char_emb_dir: None\n",
      " feature_emb_dirs: []\n",
      "\n",
      "\n",
      "Network:\n",
      " word_feature_extractor: LSTM\n",
      " use_char: True\n",
      " char_feature_extractor: CNN\n",
      " use_crf: True\n",
      "\n",
      "\n",
      "Network Hyperparameters:\n",
      " word_emb_dim: 50\n",
      " char_emb_dim: 30\n",
      " feature_emb_dims: []\n",
      " HP_char_hidden_dim: 50\n",
      " HP_hidden_dim: 200\n",
      " HP_lstm_layer: 1\n",
      " HP_bilstm: True\n",
      " HP_cnn_layer: 4\n",
      " HP_dropout: 0.5\n",
      "\n",
      "\n",
      "Training Hyperparameters:\n",
      " average_batch_loss: False\n",
      " optimizer: SGD\n",
      " iteration: 5\n",
      " batch_size: 10\n",
      " HP_lr: 0.015\n",
      " HP_lr_decayr: 0.05\n",
      " HP_clip: None\n",
      " HP_momentum: 0\n",
      " HP_l2: 1e-08\n",
      "****************************************\n",
      "\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n",
      "Epoch 0/5\n",
      " Learning rate is setted as: 0.015\n",
      " Instance 500; Time 4.6s; loss 4.64e+03; acc 5347.0/6536.0=0.8181\n",
      " Instance 1000; Time 4.0s; loss 2.857e+03; acc 11308.0/13391.0=0.8444\n",
      " Instance 1500; Time 3.8s; loss 2.77e+03; acc 17288.0/20306.0=0.8514\n",
      " Instance 2000; Time 4.7s; loss 2.175e+03; acc 23371.0/27151.0=0.8608\n",
      " Instance 2500; Time 4.6s; loss 1.838e+03; acc 29442.0/33887.0=0.8688\n",
      " Instance 3000; Time 3.4s; loss 1.549e+03; acc 35530.0/40569.0=0.8758\n",
      " Instance 3500; Time 4.2s; loss 1.286e+03; acc 42028.0/47578.0=0.8833\n",
      " Instance 4000; Time 4.7s; loss 1.194e+03; acc 48264.0/54276.0=0.8892\n",
      " Instance 4500; Time 4.2s; loss 1.084e+03; acc 54696.0/61114.0=0.895\n",
      " Instance 5000; Time 4.4s; loss 1.033e+03; acc 60803.0/67653.0=0.8987\n",
      " Instance 5500; Time 4.3s; loss 1.049e+03; acc 67481.0/74781.0=0.9024\n",
      " Instance 6000; Time 4.4s; loss 971.7; acc 73893.0/81599.0=0.9056\n",
      " Instance 6500; Time 4.7s; loss 915.3; acc 80366.0/88448.0=0.9086\n",
      " Instance 7000; Time 4.5s; loss 1.043e+03; acc 86976.0/95483.0=0.9109\n",
      " Instance 7500; Time 4.6s; loss 956.0; acc 93616.0/102502.0=0.9133\n",
      " Instance 8000; Time 3.6s; loss 998.3; acc 100018.0/109322.0=0.9149\n",
      " Instance 8500; Time 3.3s; loss 1.013e+03; acc 106142.0/115881.0=0.916\n",
      " Instance 9000; Time 3.5s; loss 886.0; acc 112565.0/122683.0=0.9175\n",
      " Instance 9500; Time 4.3s; loss 894.0; acc 119306.0/129782.0=0.9193\n",
      " Instance 10000; Time 4.5s; loss 903.3; acc 126141.0/136997.0=0.9208\n",
      " Instance 10500; Time 4.4s; loss 924.8; acc 132281.0/143526.0=0.9217\n",
      " Instance 11000; Time 4.6s; loss 875.2; acc 138597.0/150218.0=0.9226\n",
      " Instance 11500; Time 4.5s; loss 750.7; acc 145008.0/156934.0=0.924\n",
      " Instance 12000; Time 4.5s; loss 886.1; acc 151447.0/163758.0=0.9248\n",
      " Instance 12500; Time 4.5s; loss 633.9; acc 158102.0/170683.0=0.9263\n",
      " Instance 13000; Time 4.4s; loss 712.0; acc 164569.0/177453.0=0.9274\n",
      " Instance 13500; Time 4.4s; loss 782.6; acc 170989.0/184208.0=0.9282\n",
      " Instance 14000; Time 4.5s; loss 888.3; acc 177805.0/191443.0=0.9288\n",
      " Instance 14500; Time 4.4s; loss 787.8; acc 183880.0/197871.0=0.9293\n",
      " Instance 14986; Time 4.3s; loss 859.7; acc 190244.0/204566.0=0.93\n",
      " Epoch: 0 training finished. Time: 1.3e+02s; speed: 1.2e+02st/s; total loss: 38158.97933959961\n",
      "gold_num = 5913; predict_num = 5944; right_num = 5165\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Dev: time: 6.8s, speed 5.1e+02st/s; acc: 0.9745, p: 0.8689, r: 0.8735, f: 0.8712\n",
      "gold_num = 5596; predict_num = 5685; right_num = 4731\n",
      "Test: time: 6.8s, speed 6.6e+02st/s; acc: 0.9671, p: 0.8322, r: 0.8454, f: 0.8388\n",
      "\"Exceed previous best f score: -10\n",
      "Save current best model in file: ../pretrained/myModel/myModel.0.model\n",
      "Epoch 1/5\n",
      " Learning rate is setted as: 0.014285714285714285\n",
      " Instance 500; Time 4.5s; loss 842.6; acc 6771.0/7113.0=0.9519\n",
      " Instance 1000; Time 4.1s; loss 722.0; acc 13436.0/14081.0=0.9542\n",
      " Instance 1500; Time 5.3s; loss 667.0; acc 19717.0/20654.0=0.9546\n",
      " Instance 2000; Time 4.9s; loss 681.1; acc 26372.0/27612.0=0.9551\n",
      " Instance 2500; Time 4.7s; loss 750.4; acc 33456.0/34998.0=0.9559\n",
      " Instance 3000; Time 5.2s; loss 766.4; acc 40087.0/41931.0=0.956\n",
      " Instance 3500; Time 5.4s; loss 742.8; acc 46546.0/48702.0=0.9557\n",
      " Instance 4000; Time 4.1s; loss 672.6; acc 52890.0/55346.0=0.9556\n",
      " Instance 4500; Time 4.0s; loss 757.3; acc 59736.0/62488.0=0.956\n",
      " Instance 5000; Time 4.6s; loss 700.2; acc 66032.0/69088.0=0.9558\n",
      " Instance 5500; Time 3.9s; loss 753.8; acc 72463.0/75852.0=0.9553\n",
      " Instance 6000; Time 4.8s; loss 782.5; acc 79201.0/82935.0=0.955\n",
      " Instance 6500; Time 4.4s; loss 625.2; acc 85446.0/89440.0=0.9553\n",
      " Instance 7000; Time 4.5s; loss 637.3; acc 91887.0/96154.0=0.9556\n",
      " Instance 7500; Time 5.2s; loss 647.8; acc 98800.0/103335.0=0.9561\n",
      " Instance 8000; Time 8.9s; loss 758.4; acc 105263.0/110128.0=0.9558\n",
      " Instance 8500; Time 5.4s; loss 699.4; acc 111672.0/116835.0=0.9558\n",
      " Instance 9000; Time 4.5s; loss 649.1; acc 118196.0/123653.0=0.9559\n",
      " Instance 9500; Time 4.8s; loss 711.5; acc 124500.0/130248.0=0.9559\n",
      " Instance 10000; Time 4.5s; loss 596.7; acc 131232.0/137218.0=0.9564\n",
      " Instance 10500; Time 3.3s; loss 571.8; acc 136825.0/143056.0=0.9564\n",
      " Instance 11000; Time 3.6s; loss 721.7; acc 143403.0/149937.0=0.9564\n",
      " Instance 11500; Time 3.5s; loss 655.3; acc 149515.0/156334.0=0.9564\n",
      " Instance 12000; Time 4.0s; loss 593.9; acc 156147.0/163228.0=0.9566\n",
      " Instance 12500; Time 4.8s; loss 586.9; acc 162597.0/169922.0=0.9569\n",
      " Instance 13000; Time 4.8s; loss 761.0; acc 169088.0/176762.0=0.9566\n",
      " Instance 13500; Time 4.9s; loss 736.6; acc 175913.0/183872.0=0.9567\n",
      " Instance 14000; Time 4.6s; loss 632.5; acc 182277.0/190508.0=0.9568\n",
      " Instance 14500; Time 4.8s; loss 651.0; acc 189173.0/197688.0=0.9569\n",
      " Instance 14986; Time 4.7s; loss 582.9; acc 195823.0/204566.0=0.9573\n",
      " Epoch: 1 training finished. Time: 1.4e+02s; speed: 1.1e+02st/s; total loss: 20657.86602783203\n",
      "gold_num = 5913; predict_num = 5927; right_num = 5263\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Dev: time: 6.5s, speed 5.4e+02st/s; acc: 0.9774, p: 0.888, r: 0.8901, f: 0.889\n",
      "gold_num = 5596; predict_num = 5637; right_num = 4815\n",
      "Test: time: 6.5s, speed 6.3e+02st/s; acc: 0.9696, p: 0.8542, r: 0.8604, f: 0.8573\n",
      "\"Exceed previous best f score: 0.8712153158471788\n",
      "Save current best model in file: ../pretrained/myModel/myModel.1.model\n",
      "Epoch 2/5\n",
      " Learning rate is setted as: 0.013636363636363634\n",
      " Instance 500; Time 3.5s; loss 680.5; acc 6510.0/6806.0=0.9565\n",
      " Instance 1000; Time 4.5s; loss 647.8; acc 12786.0/13349.0=0.9578\n",
      " Instance 1500; Time 4.7s; loss 596.7; acc 19323.0/20143.0=0.9593\n",
      " Instance 2000; Time 4.8s; loss 547.4; acc 25996.0/27059.0=0.9607\n",
      " Instance 2500; Time 4.6s; loss 527.5; acc 32265.0/33547.0=0.9618\n",
      " Instance 3000; Time 4.4s; loss 611.7; acc 38596.0/40141.0=0.9615\n",
      " Instance 3500; Time 4.3s; loss 556.0; acc 45070.0/46858.0=0.9618\n",
      " Instance 4000; Time 3.8s; loss 473.1; acc 51556.0/53551.0=0.9627\n",
      " Instance 4500; Time 5.5s; loss 703.0; acc 58470.0/60763.0=0.9623\n",
      " Instance 5000; Time 5.6s; loss 590.5; acc 65042.0/67586.0=0.9624\n",
      " Instance 5500; Time 5.0s; loss 582.6; acc 71076.0/73897.0=0.9618\n",
      " Instance 6000; Time 5.5s; loss 610.0; acc 77767.0/80862.0=0.9617\n",
      " Instance 6500; Time 7.6s; loss 556.4; acc 84379.0/87697.0=0.9622\n",
      " Instance 7000; Time 4.7s; loss 566.9; acc 91013.0/94564.0=0.9624\n",
      " Instance 7500; Time 4.1s; loss 576.8; acc 97404.0/101188.0=0.9626\n",
      " Instance 8000; Time 3.5s; loss 572.8; acc 104393.0/108416.0=0.9629\n",
      " Instance 8500; Time 3.5s; loss 580.7; acc 110916.0/115184.0=0.9629\n",
      " Instance 9000; Time 3.9s; loss 607.7; acc 117663.0/122188.0=0.963\n",
      " Instance 9500; Time 5.3s; loss 584.6; acc 124561.0/129344.0=0.963\n",
      " Instance 10000; Time 4.4s; loss 651.0; acc 130941.0/136015.0=0.9627\n",
      " Instance 10500; Time 4.6s; loss 572.9; acc 137570.0/142887.0=0.9628\n",
      " Instance 11000; Time 4.9s; loss 504.5; acc 143860.0/149375.0=0.9631\n",
      " Instance 11500; Time 4.6s; loss 485.2; acc 150604.0/156328.0=0.9634\n",
      " Instance 12000; Time 4.5s; loss 622.0; acc 157481.0/163453.0=0.9635\n",
      " Instance 12500; Time 5.2s; loss 567.0; acc 164109.0/170308.0=0.9636\n",
      " Instance 13000; Time 5.1s; loss 582.0; acc 170638.0/177089.0=0.9636\n",
      " Instance 13500; Time 4.4s; loss 557.6; acc 177321.0/184029.0=0.9635\n",
      " Instance 14000; Time 4.5s; loss 430.8; acc 184021.0/190913.0=0.9639\n",
      " Instance 14500; Time 5.0s; loss 632.5; acc 190988.0/198146.0=0.9639\n",
      " Instance 14986; Time 5.2s; loss 534.4; acc 197177.0/204566.0=0.9639\n",
      " Epoch: 2 training finished. Time: 1.4e+02s; speed: 1.1e+02st/s; total loss: 17312.029815673828\n",
      "gold_num = 5913; predict_num = 5880; right_num = 5323\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Dev: time: 6.6s, speed 5.3e+02st/s; acc: 0.9801, p: 0.9053, r: 0.9002, f: 0.9027\n",
      "gold_num = 5596; predict_num = 5581; right_num = 4859\n",
      "Test: time: 6.6s, speed 6.2e+02st/s; acc: 0.9717, p: 0.8706, r: 0.8683, f: 0.8695\n",
      "\"Exceed previous best f score: 0.8890202702702704\n",
      "Save current best model in file: ../pretrained/myModel/myModel.2.model\n",
      "Epoch 3/5\n",
      " Learning rate is setted as: 0.013043478260869566\n",
      " Instance 500; Time 4.5s; loss 588.5; acc 6470.0/6726.0=0.9619\n",
      " Instance 1000; Time 4.3s; loss 556.9; acc 12846.0/13341.0=0.9629\n",
      " Instance 1500; Time 4.6s; loss 621.5; acc 19591.0/20368.0=0.9619\n",
      " Instance 2000; Time 4.7s; loss 460.5; acc 26262.0/27225.0=0.9646\n",
      " Instance 2500; Time 5.0s; loss 556.2; acc 33103.0/34318.0=0.9646\n",
      " Instance 3000; Time 4.0s; loss 415.7; acc 39511.0/40914.0=0.9657\n",
      " Instance 3500; Time 4.5s; loss 617.1; acc 46550.0/48215.0=0.9655\n",
      " Instance 4000; Time 5.4s; loss 455.0; acc 52998.0/54860.0=0.9661\n",
      " Instance 4500; Time 4.6s; loss 535.4; acc 59204.0/61279.0=0.9661\n",
      " Instance 5000; Time 4.6s; loss 485.2; acc 65800.0/68060.0=0.9668\n",
      " Instance 5500; Time 4.6s; loss 580.4; acc 72525.0/75054.0=0.9663\n",
      " Instance 6000; Time 5.3s; loss 452.3; acc 79688.0/82407.0=0.967\n",
      " Instance 6500; Time 5.4s; loss 486.2; acc 85936.0/88889.0=0.9668\n",
      " Instance 7000; Time 4.8s; loss 581.0; acc 92535.0/95741.0=0.9665\n",
      " Instance 7500; Time 5.0s; loss 513.4; acc 99057.0/102484.0=0.9666\n",
      " Instance 8000; Time 6.4s; loss 569.7; acc 105539.0/109197.0=0.9665\n",
      " Instance 8500; Time 5.4s; loss 561.2; acc 112639.0/116554.0=0.9664\n",
      " Instance 9000; Time 5.2s; loss 556.9; acc 119267.0/123399.0=0.9665\n",
      " Instance 9500; Time 4.7s; loss 493.1; acc 125836.0/130188.0=0.9666\n",
      " Instance 10000; Time 4.5s; loss 475.1; acc 132181.0/136751.0=0.9666\n",
      " Instance 10500; Time 4.0s; loss 532.6; acc 138877.0/143677.0=0.9666\n",
      " Instance 11000; Time 4.1s; loss 448.5; acc 145610.0/150596.0=0.9669\n",
      " Instance 11500; Time 4.0s; loss 452.3; acc 152202.0/157379.0=0.9671\n",
      " Instance 12000; Time 3.7s; loss 525.0; acc 159361.0/164778.0=0.9671\n",
      " Instance 12500; Time 4.4s; loss 508.9; acc 165678.0/171297.0=0.9672\n",
      " Instance 13000; Time 5.9s; loss 586.0; acc 172148.0/178000.0=0.9671\n",
      " Instance 13500; Time 4.6s; loss 528.8; acc 178760.0/184825.0=0.9672\n",
      " Instance 14000; Time 4.4s; loss 488.7; acc 185282.0/191550.0=0.9673\n",
      " Instance 14500; Time 5.2s; loss 547.6; acc 191481.0/197983.0=0.9672\n",
      " Instance 14986; Time 4.9s; loss 498.6; acc 197864.0/204566.0=0.9672\n",
      " Epoch: 3 training finished. Time: 1.4e+02s; speed: 1.1e+02st/s; total loss: 15678.241821289062\n",
      "gold_num = 5913; predict_num = 5921; right_num = 5369\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Dev: time: 8.0s, speed 4.4e+02st/s; acc: 0.981, p: 0.9068, r: 0.908, f: 0.9074\n",
      "gold_num = 5596; predict_num = 5622; right_num = 4930\n",
      "Test: time: 8.0s, speed 5.9e+02st/s; acc: 0.9728, p: 0.8769, r: 0.881, f: 0.8789\n",
      "\"Exceed previous best f score: 0.9027389129144407\n",
      "Save current best model in file: ../pretrained/myModel/myModel.3.model\n",
      "Epoch 4/5\n",
      " Learning rate is setted as: 0.0125\n",
      " Instance 500; Time 3.7s; loss 482.3; acc 6641.0/6844.0=0.9703\n",
      " Instance 1000; Time 4.7s; loss 424.5; acc 12969.0/13362.0=0.9706\n",
      " Instance 1500; Time 4.6s; loss 457.1; acc 19904.0/20491.0=0.9714\n",
      " Instance 2000; Time 4.6s; loss 531.6; acc 27041.0/27860.0=0.9706\n",
      " Instance 2500; Time 4.7s; loss 497.5; acc 33297.0/34316.0=0.9703\n",
      " Instance 3000; Time 4.4s; loss 425.0; acc 40374.0/41562.0=0.9714\n",
      " Instance 3500; Time 4.3s; loss 444.4; acc 46681.0/48069.0=0.9711\n",
      " Instance 4000; Time 4.6s; loss 441.9; acc 53729.0/55300.0=0.9716\n",
      " Instance 4500; Time 7.8s; loss 531.5; acc 60364.0/62157.0=0.9712\n",
      " Instance 5000; Time 5.0s; loss 496.6; acc 66687.0/68702.0=0.9707\n",
      " Instance 5500; Time 4.8s; loss 449.5; acc 73087.0/75305.0=0.9705\n",
      " Instance 6000; Time 4.6s; loss 462.1; acc 79564.0/81967.0=0.9707\n",
      " Instance 6500; Time 5.9s; loss 549.5; acc 85994.0/88639.0=0.9702\n",
      " Instance 7000; Time 4.9s; loss 558.3; acc 92789.0/95658.0=0.97\n",
      " Instance 7500; Time 4.6s; loss 534.3; acc 99260.0/102355.0=0.9698\n",
      " Instance 8000; Time 3.9s; loss 486.1; acc 105813.0/109113.0=0.9698\n",
      " Instance 8500; Time 4.4s; loss 505.1; acc 112533.0/116059.0=0.9696\n",
      " Instance 9000; Time 4.8s; loss 427.3; acc 119432.0/123143.0=0.9699\n",
      " Instance 9500; Time 5.0s; loss 478.4; acc 125644.0/129560.0=0.9698\n",
      " Instance 10000; Time 5.1s; loss 436.2; acc 132184.0/136273.0=0.97\n",
      " Instance 10500; Time 4.9s; loss 459.5; acc 138832.0/143111.0=0.9701\n",
      " Instance 11000; Time 4.5s; loss 420.7; acc 145097.0/149564.0=0.9701\n",
      " Instance 11500; Time 4.4s; loss 426.0; acc 151732.0/156379.0=0.9703\n",
      " Instance 12000; Time 4.6s; loss 478.2; acc 158197.0/163067.0=0.9701\n",
      " Instance 12500; Time 4.8s; loss 486.4; acc 165022.0/170081.0=0.9703\n",
      " Instance 13000; Time 4.9s; loss 528.0; acc 171675.0/176960.0=0.9701\n",
      " Instance 13500; Time 4.5s; loss 443.6; acc 178154.0/183633.0=0.9702\n",
      " Instance 14000; Time 4.9s; loss 429.8; acc 184716.0/190370.0=0.9703\n",
      " Instance 14500; Time 4.7s; loss 595.1; acc 191596.0/197518.0=0.97\n",
      " Instance 14986; Time 4.5s; loss 457.6; acc 198453.0/204566.0=0.9701\n",
      " Epoch: 4 training finished. Time: 1.4e+02s; speed: 1e+02st/s; total loss: 14344.028076171875\n",
      "gold_num = 5913; predict_num = 5881; right_num = 5360\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Dev: time: 6.3s, speed 5.5e+02st/s; acc: 0.9814, p: 0.9114, r: 0.9065, f: 0.9089\n",
      "gold_num = 5596; predict_num = 5598; right_num = 4934\n",
      "Test: time: 6.3s, speed 5.6e+02st/s; acc: 0.9739, p: 0.8814, r: 0.8817, f: 0.8815\n",
      "\"Exceed previous best f score: 0.9073854994084839\n",
      "Save current best model in file: ../pretrained/myModel/myModel.4.model\n",
      "Training done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9089367474987281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to decode a new input from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model weights from file ../pretrained/myModel/myModel.1.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/myModel/myModel.xpt'\n",
    "path2model = '../pretrained/myModel/myModel.1.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.load_model_dir = path2model\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2write = out_folder + os.path.basename(os.path.splitext(file_name)[0]) + '.out'\n",
    "# open and return the text of the file\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.091 s\n",
      "Decoding speed: 319.2 st/s\n",
      "[['B-ORG', 'O', 'B-PER', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'O', 'B-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O'], ['B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O'], ['B-ORG', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'O', 'B-PER', 'I-PER', 'O']]\n",
      "['B-ORG O B-PER O O B-PER O O O B-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O O O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER O B-PER O B-PER O O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O O O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O B-PER I-PER O B-PER O B-PER I-PER O B-PER O B-PER I-PER O\\n', 'B-ORG O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O O O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-ORG O O O B-PER O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O O B-PER I-PER O\\n', 'B-PER I-PER O B-PER I-PER O B-PER O B-PER I-PER O B-PER I-PER O\\n', 'B-ORG O O B-PER I-PER O O B-PER I-PER O B-PER I-PER O O B-PER I-PER O B-PER O B-PER O O O B-PER I-PER O B-PER I-PER O B-PER O O B-PER O B-PER I-PER O B-PER O O B-PER I-PER O\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training informations of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
