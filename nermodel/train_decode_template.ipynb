{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import train, data_initialization, build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a model on conll2003 shared task training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2train = '../conll2003/train.conll2003'\n",
    "path2dev = '../conll2003/dev.conll2003'\n",
    "path2test = '../conll2003/test.conll2003'\n",
    "path2model = '../pretrained/myModel/myModel'\n",
    "modelDir = os.path.join(*path2model.split('/')[:-1])\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "path2emb = '../pretrained/glove.6B.50d.txt'\n",
    "confdict = {# IO\n",
    "            'train_dir':path2train,\n",
    "            'dev_dir':path2dev,\n",
    "            'test_dir':path2test,\n",
    "            'model_dir':path2model,\n",
    "            # Embeddings\n",
    "            'word_emb_dir':path2emb,\n",
    "            'char_emb_dir':None,\n",
    "            'word_emb_dim':50,\n",
    "            'char_emb_dim':30,\n",
    "            # Network\n",
    "            'use_crf':True,\n",
    "            'use_char':True,\n",
    "            'use_feats': False,\n",
    "            'word_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            'char_feature_extractor':'LSTM', # choose CNN/LSTM/GRU\n",
    "            # HP\n",
    "            'HP_cnn_layer':4 ,\n",
    "            'HP_char_hidden_dim':50,\n",
    "            'HP_hidden_dim':200,\n",
    "            'HP_dropout':0.5,\n",
    "            'HP_lstm_layer':1,\n",
    "            'HP_bilstm':True,\n",
    "            'HP_lr':0.015,\n",
    "            # training\n",
    "            'optimizer':'SGD',\n",
    "            'batch_size':10,\n",
    "            'iteration':5\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained word embedding, norm False, dir: ../pretrained/glove.6B.50d.txt\n",
      "Embedding: \n",
      " pretrain words: 400000, perfect_match: 14618, case_match: 11722, oov: 3951\n",
      "Training model...\n",
      "****************************************\n",
      "----------Data summary:----------\n",
      "\n",
      " HP_gpu: False\n",
      " MAX_SENTENCE_LENTGH: 1000\n",
      " number_normalized: False\n",
      " word_alphabet: 30292\n",
      " char_alphabet_size: 87\n",
      " label_alphabet_size: 10\n",
      " load_model_dir: None\n",
      "\n",
      "\n",
      "I/O:\n",
      " tagScheme: BIO\n",
      " train_dir: ../conll2003/train.conll2003\n",
      " dev_dir: ../conll2003/dev.conll2003\n",
      " test_dir: ../conll2003/test.conll2003\n",
      " raw_dir: None\n",
      " dset_dir: None\n",
      " word_emb_dir: ../pretrained/glove.6B.50d.txt\n",
      " char_emb_dir: None\n",
      " feature_emb_dirs: []\n",
      "\n",
      "\n",
      "Network:\n",
      " word_feature_extractor: LSTM\n",
      " use_char: True\n",
      " char_feature_extractor: LSTM\n",
      " use_crf: True\n",
      "\n",
      "\n",
      "Network Hyperparameters:\n",
      " word_emb_dim: 50\n",
      " char_emb_dim: 30\n",
      " feature_emb_dims: []\n",
      " HP_char_hidden_dim: 50\n",
      " HP_hidden_dim: 200\n",
      " HP_lstm_layer: 1\n",
      " HP_bilstm: True\n",
      " HP_cnn_layer: 4\n",
      " HP_dropout: 0.5\n",
      "\n",
      "\n",
      "Training Hyperparameters:\n",
      " average_batch_loss: False\n",
      " optimizer: SGD\n",
      " iteration: 5\n",
      " batch_size: 10\n",
      " HP_lr: 0.015\n",
      " HP_lr_decayr: 0.05\n",
      " HP_clip: None\n",
      " HP_momentum: 0\n",
      " HP_l2: 1e-08\n",
      "****************************************\n",
      "\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  LSTM\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "build char sequence feature extractor: LSTM ...\n",
      "build CRF...\n",
      "Epoch 0/5\n",
      " Learning rate is setted as: 0.015\n",
      " Instance 500; Time 5.7s; loss 4.702e+03; acc 5761.0/6983.0=0.825\n",
      " Instance 1000; Time 5.2s; loss 3.303e+03; acc 11667.0/13953.0=0.8362\n",
      " Instance 1500; Time 4.9s; loss 2.681e+03; acc 17475.0/20675.0=0.8452\n",
      " Instance 2000; Time 5.2s; loss 2.392e+03; acc 23632.0/27694.0=0.8533\n",
      " Instance 2500; Time 5.3s; loss 2.157e+03; acc 29651.0/34437.0=0.861\n",
      " Instance 3000; Time 4.9s; loss 2.128e+03; acc 35359.0/40922.0=0.8641\n",
      " Instance 3500; Time 4.2s; loss 1.78e+03; acc 41443.0/47667.0=0.8694\n",
      " Instance 4000; Time 4.0s; loss 1.842e+03; acc 47702.0/54614.0=0.8734\n",
      " Instance 4500; Time 5.0s; loss 1.315e+03; acc 53498.0/60908.0=0.8783\n",
      " Instance 5000; Time 5.2s; loss 1.152e+03; acc 59728.0/67590.0=0.8837\n",
      " Instance 5500; Time 4.1s; loss 1.246e+03; acc 65947.0/74279.0=0.8878\n",
      " Instance 6000; Time 3.8s; loss 1.147e+03; acc 71829.0/80627.0=0.8909\n",
      " Instance 6500; Time 4.6s; loss 1.148e+03; acc 78130.0/87380.0=0.8941\n",
      " Instance 7000; Time 5.0s; loss 1.146e+03; acc 84564.0/94287.0=0.8969\n",
      " Instance 7500; Time 5.0s; loss 1.012e+03; acc 90628.0/100774.0=0.8993\n",
      " Instance 8000; Time 5.4s; loss 1.011e+03; acc 97662.0/108238.0=0.9023\n",
      " Instance 8500; Time 5.0s; loss 994.5; acc 103928.0/114920.0=0.9044\n",
      " Instance 9000; Time 5.2s; loss 876.8; acc 110586.0/121942.0=0.9069\n",
      " Instance 9500; Time 4.9s; loss 977.7; acc 116513.0/128296.0=0.9082\n",
      " Instance 10000; Time 5.4s; loss 959.1; acc 123444.0/135643.0=0.9101\n",
      " Instance 10500; Time 5.1s; loss 902.6; acc 130192.0/142766.0=0.9119\n",
      " Instance 11000; Time 5.4s; loss 944.4; acc 137247.0/150204.0=0.9137\n",
      " Instance 11500; Time 5.3s; loss 891.3; acc 143821.0/157151.0=0.9152\n",
      " Instance 12000; Time 5.1s; loss 824.4; acc 150308.0/163993.0=0.9166\n",
      " Instance 12500; Time 5.3s; loss 828.7; acc 156710.0/170747.0=0.9178\n",
      " Instance 13000; Time 4.8s; loss 837.8; acc 162991.0/177376.0=0.9189\n",
      " Instance 13500; Time 3.8s; loss 793.0; acc 169496.0/184234.0=0.92\n",
      " Instance 14000; Time 3.8s; loss 698.3; acc 175825.0/190856.0=0.9212\n",
      " Instance 14500; Time 3.9s; loss 861.3; acc 182133.0/197497.0=0.9222\n",
      " Instance 14986; Time 3.9s; loss 839.9; acc 188839.0/204566.0=0.9231\n",
      " Epoch: 0 training finished. Time: 1.4e+02s; speed: 1e+02st/s; total loss: 42391.217361450195\n",
      "gold_num = 5913; predict_num = 5983; right_num = 5127\n",
      "Dev: time: 8.8s, speed 4e+02st/s; acc: 0.9725, p: 0.8569, r: 0.8671, f: 0.862\n",
      "gold_num = 5596; predict_num = 5722; right_num = 4661\n",
      "Test: time: 8.8s, speed 4.5e+02st/s; acc: 0.9637, p: 0.8146, r: 0.8329, f: 0.8236\n",
      "\"Exceed previous best f score: -10\n",
      "Save current best model in file: ../pretrained/myModel/myModel.0.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 1/5\n",
      " Learning rate is setted as: 0.014285714285714285\n",
      " Instance 500; Time 5.1s; loss 794.3; acc 6579.0/6931.0=0.9492\n",
      " Instance 1000; Time 5.3s; loss 748.1; acc 13363.0/14050.0=0.9511\n",
      " Instance 1500; Time 5.1s; loss 743.8; acc 19817.0/20792.0=0.9531\n",
      " Instance 2000; Time 4.6s; loss 727.5; acc 26618.0/27902.0=0.954\n",
      " Instance 2500; Time 3.8s; loss 808.7; acc 33181.0/34808.0=0.9533\n",
      " Instance 3000; Time 4.2s; loss 742.7; acc 39498.0/41458.0=0.9527\n",
      " Instance 3500; Time 5.4s; loss 684.3; acc 45713.0/47957.0=0.9532\n",
      " Instance 4000; Time 5.2s; loss 661.9; acc 51912.0/54411.0=0.9541\n",
      " Instance 4500; Time 5.6s; loss 751.8; acc 58684.0/61489.0=0.9544\n",
      " Instance 5000; Time 4.1s; loss 628.6; acc 64787.0/67882.0=0.9544\n",
      " Instance 5500; Time 4.2s; loss 642.2; acc 71339.0/74712.0=0.9549\n",
      " Instance 6000; Time 4.3s; loss 733.0; acc 78496.0/82178.0=0.9552\n",
      " Instance 6500; Time 4.8s; loss 673.3; acc 85237.0/89184.0=0.9557\n",
      " Instance 7000; Time 4.5s; loss 609.3; acc 91428.0/95632.0=0.956\n",
      " Instance 7500; Time 5.8s; loss 663.2; acc 97658.0/102147.0=0.9561\n",
      " Instance 8000; Time 4.2s; loss 731.9; acc 104393.0/109188.0=0.9561\n",
      " Instance 8500; Time 4.2s; loss 691.3; acc 111140.0/116237.0=0.9561\n",
      " Instance 9000; Time 4.5s; loss 572.2; acc 117677.0/123021.0=0.9566\n",
      " Instance 9500; Time 3.8s; loss 539.6; acc 124242.0/129810.0=0.9571\n",
      " Instance 10000; Time 3.8s; loss 666.9; acc 130623.0/136470.0=0.9572\n",
      " Instance 10500; Time 3.8s; loss 614.0; acc 137207.0/143320.0=0.9573\n",
      " Instance 11000; Time 3.8s; loss 694.0; acc 143681.0/150096.0=0.9573\n",
      " Instance 11500; Time 3.6s; loss 615.3; acc 150042.0/156732.0=0.9573\n",
      " Instance 12000; Time 3.9s; loss 653.6; acc 156902.0/163870.0=0.9575\n",
      " Instance 12500; Time 3.8s; loss 725.4; acc 163582.0/170869.0=0.9574\n",
      " Instance 13000; Time 3.9s; loss 691.1; acc 169868.0/177432.0=0.9574\n",
      " Instance 13500; Time 3.8s; loss 690.8; acc 176515.0/184373.0=0.9574\n",
      " Instance 14000; Time 3.9s; loss 647.8; acc 182885.0/191025.0=0.9574\n",
      " Instance 14500; Time 3.8s; loss 669.3; acc 189272.0/197709.0=0.9573\n",
      " Instance 14986; Time 3.8s; loss 676.7; acc 195844.0/204566.0=0.9574\n",
      " Epoch: 1 training finished. Time: 1.3e+02s; speed: 1.1e+02st/s; total loss: 20492.655242919922\n",
      "gold_num = 5913; predict_num = 5927; right_num = 5294\n",
      "Dev: time: 8.8s, speed 4e+02st/s; acc: 0.9783, p: 0.8932, r: 0.8953, f: 0.8943\n",
      "gold_num = 5596; predict_num = 5672; right_num = 4837\n",
      "Test: time: 8.8s, speed 4.4e+02st/s; acc: 0.969, p: 0.8528, r: 0.8644, f: 0.8585\n",
      "\"Exceed previous best f score: 0.8619704102219233\n",
      "Save current best model in file: ../pretrained/myModel/myModel.1.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 2/5\n",
      " Learning rate is setted as: 0.013636363636363634\n",
      " Instance 500; Time 5.1s; loss 598.5; acc 6488.0/6742.0=0.9623\n",
      " Instance 1000; Time 5.0s; loss 586.3; acc 13180.0/13721.0=0.9606\n",
      " Instance 1500; Time 5.2s; loss 620.0; acc 19708.0/20513.0=0.9608\n",
      " Instance 2000; Time 5.2s; loss 521.5; acc 26129.0/27158.0=0.9621\n",
      " Instance 2500; Time 5.0s; loss 557.4; acc 32289.0/33548.0=0.9625\n",
      " Instance 3000; Time 5.1s; loss 651.5; acc 38672.0/40204.0=0.9619\n",
      " Instance 3500; Time 5.2s; loss 519.7; acc 45508.0/47265.0=0.9628\n",
      " Instance 4000; Time 6.0s; loss 641.1; acc 52006.0/54032.0=0.9625\n",
      " Instance 4500; Time 5.4s; loss 576.3; acc 58823.0/61107.0=0.9626\n",
      " Instance 5000; Time 5.0s; loss 562.1; acc 65158.0/67678.0=0.9628\n",
      " Instance 5500; Time 5.1s; loss 551.6; acc 71883.0/74644.0=0.963\n",
      " Instance 6000; Time 5.2s; loss 597.1; acc 78404.0/81415.0=0.963\n",
      " Instance 6500; Time 5.0s; loss 627.8; acc 84850.0/88133.0=0.9627\n",
      " Instance 7000; Time 5.3s; loss 610.6; acc 91665.0/95192.0=0.9629\n",
      " Instance 7500; Time 5.3s; loss 598.5; acc 98415.0/102207.0=0.9629\n",
      " Instance 8000; Time 5.2s; loss 582.9; acc 104899.0/108931.0=0.963\n",
      " Instance 8500; Time 5.1s; loss 622.1; acc 111433.0/115720.0=0.963\n",
      " Instance 9000; Time 6.0s; loss 678.2; acc 118012.0/122586.0=0.9627\n",
      " Instance 9500; Time 5.4s; loss 561.6; acc 124672.0/129491.0=0.9628\n",
      " Instance 10000; Time 5.0s; loss 604.2; acc 131094.0/136166.0=0.9628\n",
      " Instance 10500; Time 5.1s; loss 591.1; acc 137667.0/142988.0=0.9628\n",
      " Instance 11000; Time 5.2s; loss 540.3; acc 144197.0/149737.0=0.963\n",
      " Instance 11500; Time 5.2s; loss 578.6; acc 151067.0/156838.0=0.9632\n",
      " Instance 12000; Time 5.1s; loss 580.5; acc 157776.0/163789.0=0.9633\n",
      " Instance 12500; Time 5.3s; loss 564.5; acc 164584.0/170837.0=0.9634\n",
      " Instance 13000; Time 5.0s; loss 515.3; acc 170983.0/177429.0=0.9637\n",
      " Instance 13500; Time 5.2s; loss 540.9; acc 177643.0/184300.0=0.9639\n",
      " Instance 14000; Time 5.2s; loss 565.0; acc 184216.0/191129.0=0.9638\n",
      " Instance 14500; Time 5.1s; loss 510.6; acc 190934.0/198048.0=0.9641\n",
      " Instance 14986; Time 5.0s; loss 620.5; acc 197204.0/204566.0=0.964\n",
      " Epoch: 2 training finished. Time: 1.6e+02s; speed: 9.6e+01st/s; total loss: 17476.1513671875\n",
      "gold_num = 5913; predict_num = 5887; right_num = 5329\n",
      "Dev: time: 9.0s, speed 3.9e+02st/s; acc: 0.98, p: 0.9052, r: 0.9012, f: 0.9032\n",
      "gold_num = 5596; predict_num = 5604; right_num = 4882\n",
      "Test: time: 9.0s, speed 4.5e+02st/s; acc: 0.9717, p: 0.8712, r: 0.8724, f: 0.8718\n",
      "\"Exceed previous best f score: 0.8942567567567569\n",
      "Save current best model in file: ../pretrained/myModel/myModel.2.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 3/5\n",
      " Learning rate is setted as: 0.013043478260869566\n",
      " Instance 500; Time 5.0s; loss 507.8; acc 6336.0/6551.0=0.9672\n",
      " Instance 1000; Time 5.7s; loss 526.8; acc 12720.0/13143.0=0.9678\n",
      " Instance 1500; Time 5.5s; loss 503.7; acc 19427.0/20057.0=0.9686\n",
      " Instance 2000; Time 5.3s; loss 479.6; acc 25987.0/26815.0=0.9691\n",
      " Instance 2500; Time 5.1s; loss 509.6; acc 32474.0/33505.0=0.9692\n",
      " Instance 3000; Time 5.1s; loss 542.0; acc 38771.0/40015.0=0.9689\n",
      " Instance 3500; Time 5.1s; loss 498.5; acc 45241.0/46701.0=0.9687\n",
      " Instance 4000; Time 5.3s; loss 488.9; acc 51825.0/53495.0=0.9688\n",
      " Instance 4500; Time 5.2s; loss 470.0; acc 58260.0/60133.0=0.9689\n",
      " Instance 5000; Time 5.0s; loss 505.1; acc 64859.0/66959.0=0.9686\n",
      " Instance 5500; Time 6.0s; loss 536.2; acc 71579.0/73912.0=0.9684\n",
      " Instance 6000; Time 5.3s; loss 501.2; acc 78067.0/80592.0=0.9687\n",
      " Instance 6500; Time 5.3s; loss 586.4; acc 84934.0/87675.0=0.9687\n",
      " Instance 7000; Time 5.3s; loss 539.1; acc 91392.0/94347.0=0.9687\n",
      " Instance 7500; Time 5.7s; loss 498.9; acc 97710.0/100885.0=0.9685\n",
      " Instance 8000; Time 5.1s; loss 492.6; acc 104446.0/107829.0=0.9686\n",
      " Instance 8500; Time 4.9s; loss 472.3; acc 110716.0/114305.0=0.9686\n",
      " Instance 9000; Time 5.4s; loss 537.7; acc 118065.0/121888.0=0.9686\n",
      " Instance 9500; Time 5.0s; loss 523.3; acc 124529.0/128595.0=0.9684\n",
      " Instance 10000; Time 4.1s; loss 500.3; acc 131006.0/135281.0=0.9684\n",
      " Instance 10500; Time 5.4s; loss 522.2; acc 138171.0/142677.0=0.9684\n",
      " Instance 11000; Time 5.4s; loss 538.6; acc 145137.0/149880.0=0.9684\n",
      " Instance 11500; Time 5.3s; loss 533.1; acc 151749.0/156743.0=0.9681\n",
      " Instance 12000; Time 5.1s; loss 467.1; acc 158430.0/163623.0=0.9683\n",
      " Instance 12500; Time 5.2s; loss 494.5; acc 164907.0/170314.0=0.9683\n",
      " Instance 13000; Time 4.9s; loss 562.4; acc 171792.0/177432.0=0.9682\n",
      " Instance 13500; Time 5.1s; loss 440.8; acc 178431.0/184235.0=0.9685\n",
      " Instance 14000; Time 5.3s; loss 605.2; acc 184917.0/190950.0=0.9684\n",
      " Instance 14500; Time 4.6s; loss 520.5; acc 191473.0/197728.0=0.9684\n",
      " Instance 14986; Time 4.1s; loss 569.4; acc 198076.0/204566.0=0.9683\n",
      " Epoch: 3 training finished. Time: 1.6e+02s; speed: 9.7e+01st/s; total loss: 15473.8046875\n",
      "gold_num = 5913; predict_num = 5860; right_num = 5392\n",
      "Dev: time: 8.7s, speed 4e+02st/s; acc: 0.9826, p: 0.9201, r: 0.9119, f: 0.916\n",
      "gold_num = 5596; predict_num = 5593; right_num = 4919\n",
      "Test: time: 8.7s, speed 4.3e+02st/s; acc: 0.9736, p: 0.8795, r: 0.879, f: 0.8793\n",
      "\"Exceed previous best f score: 0.9032203389830509\n",
      "Save current best model in file: ../pretrained/myModel/myModel.3.model\n",
      "Save informations about model in file: ../pretrained/myModel/myModel.infos\n",
      "Epoch 4/5\n",
      " Learning rate is setted as: 0.0125\n",
      " Instance 500; Time 4.3s; loss 437.2; acc 6543.0/6735.0=0.9715\n",
      " Instance 1000; Time 5.9s; loss 398.8; acc 13306.0/13660.0=0.9741\n",
      " Instance 1500; Time 5.3s; loss 468.4; acc 20044.0/20610.0=0.9725\n",
      " Instance 2000; Time 5.1s; loss 438.9; acc 26479.0/27226.0=0.9726\n",
      " Instance 2500; Time 5.1s; loss 544.9; acc 33152.0/34110.0=0.9719\n",
      " Instance 3000; Time 5.1s; loss 484.8; acc 40069.0/41242.0=0.9716\n",
      " Instance 3500; Time 5.1s; loss 442.1; acc 46623.0/47991.0=0.9715\n",
      " Instance 4000; Time 5.2s; loss 416.3; acc 53521.0/55062.0=0.972\n",
      " Instance 4500; Time 5.3s; loss 461.3; acc 59897.0/61643.0=0.9717\n",
      " Instance 5000; Time 5.1s; loss 521.5; acc 66318.0/68272.0=0.9714\n",
      " Instance 5500; Time 5.2s; loss 557.7; acc 72921.0/75124.0=0.9707\n",
      " Instance 6000; Time 5.2s; loss 457.0; acc 79735.0/82162.0=0.9705\n",
      " Instance 6500; Time 5.2s; loss 544.3; acc 86325.0/88987.0=0.9701\n",
      " Instance 7000; Time 5.2s; loss 531.9; acc 93202.0/96112.0=0.9697\n",
      " Instance 7500; Time 6.1s; loss 442.3; acc 99860.0/102948.0=0.97\n",
      " Instance 8000; Time 5.5s; loss 462.9; acc 106553.0/109845.0=0.97\n",
      " Instance 8500; Time 5.4s; loss 526.2; acc 113189.0/116681.0=0.9701\n",
      " Instance 9000; Time 5.1s; loss 419.9; acc 119436.0/123122.0=0.9701\n",
      " Instance 9500; Time 5.2s; loss 477.2; acc 126005.0/129893.0=0.9701\n",
      " Instance 10000; Time 5.3s; loss 471.1; acc 132663.0/136745.0=0.9701\n",
      " Instance 10500; Time 5.3s; loss 532.6; acc 139389.0/143691.0=0.9701\n",
      " Instance 11000; Time 5.7s; loss 438.4; acc 145597.0/150085.0=0.9701\n",
      " Instance 11500; Time 5.4s; loss 446.0; acc 152318.0/157001.0=0.9702\n",
      " Instance 12000; Time 5.2s; loss 475.3; acc 158831.0/163713.0=0.9702\n",
      " Instance 12500; Time 5.5s; loss 533.2; acc 165693.0/170779.0=0.9702\n",
      " Instance 13000; Time 5.8s; loss 524.8; acc 172007.0/177305.0=0.9701\n",
      " Instance 13500; Time 5.2s; loss 528.9; acc 178519.0/184032.0=0.97\n",
      " Instance 14000; Time 5.4s; loss 416.7; acc 185338.0/191024.0=0.9702\n",
      " Instance 14500; Time 5.5s; loss 488.4; acc 192408.0/198288.0=0.9703\n",
      " Instance 14986; Time 4.9s; loss 421.3; acc 198520.0/204566.0=0.9704\n",
      " Epoch: 4 training finished. Time: 1.6e+02s; speed: 9.4e+01st/s; total loss: 14310.468719482422\n",
      "gold_num = 5913; predict_num = 5936; right_num = 5411\n",
      "Dev: time: 9.5s, speed 3.7e+02st/s; acc: 0.9824, p: 0.9116, r: 0.9151, f: 0.9133\n",
      "gold_num = 5596; predict_num = 5674; right_num = 4947\n",
      "Test: time: 9.5s, speed 4.4e+02st/s; acc: 0.9729, p: 0.8719, r: 0.884, f: 0.8779\n",
      "Training done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9159942240720291"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialization of data object and training (equivalent to main.myTrain(confdict))\n",
    "data = Data()\n",
    "data.read_config(confdict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data_initialization(data)\n",
    "data.generate_instance('train')\n",
    "data.generate_instance('dev')\n",
    "data.generate_instance('test')\n",
    "data.build_pretrain_emb()\n",
    "train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to decode a new input from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model weights from file ../pretrained/myModel/myModel.4.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  LSTM\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "build char sequence feature extractor: LSTM ...\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/myModel/myModel.xpt'\n",
    "path2model = '../pretrained/myModel/myModel.4.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.load_model_dir = path2model\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/inputs/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2write = out_folder + os.path.basename(os.path.splitext(file_name)[0]) + '.out'\n",
    "# open and return the text of the file\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.11 s\n",
      "Decoding speed: 267.6 st/s\n",
      "[['B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-MISC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], ['O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-MISC', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-LOC', 'O'], ['O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-MISC', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-MISC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O'], ['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O'], ['O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-MISC', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'I-ORG']]\n",
      "['B-LOC O B-MISC O O O O O O O O B-MISC I-MISC O B-MISC O B-ORG O O O O O O O O O O O O O O O O B-LOC O B-LOC I-LOC O O O O O O O O O O\\n', 'O O O O O O O B-LOC O O O B-LOC I-LOC O O B-MISC I-MISC O O B-LOC I-LOC O O O O B-LOC O O B-LOC I-LOC O\\n', 'O O O O B-MISC I-MISC O B-LOC I-LOC O O O O O B-LOC O B-LOC O B-MISC O O\\n', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O O O O O\\n', 'O O O O O B-LOC O B-LOC O B-ORG O B-ORG O B-ORG O B-LOC O\\n', 'O O B-MISC I-MISC O O O O O B-LOC O O O O B-MISC O O B-MISC O O\\n', 'B-LOC O O O O O B-MISC O O O O O O O B-ORG I-ORG O O O O O O B-LOC O B-LOC O\\n', 'B-LOC O O O O B-MISC O O O B-ORG I-ORG I-ORG O O O O O B-MISC I-MISC I-MISC I-MISC O O O O O O\\n', 'O O B-MISC O B-MISC O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG O O\\n', 'B-LOC O B-LOC O O O O O O O O O O B-ORG I-ORG O\\n', 'O O O O O O O O O O B-MISC I-MISC O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O O O B-MISC O O O O O O O O O O O\\n', 'O O O O B-MISC O O O O O B-MISC I-MISC I-MISC O\\n', 'O O B-MISC I-MISC O O O O O B-LOC O\\n', 'O O O O O B-LOC O B-LOC O O O O O O O O O O O O B-MISC I-MISC I-MISC O O O\\n', 'B-LOC O O O O O B-MISC I-MISC O O O O O O O O O O O O O B-MISC I-MISC O B-MISC I-MISC I-MISC O O O O O O O B-MISC O O O O\\n', 'O O O O O O B-LOC I-LOC O O O O O O O O O O B-MISC I-MISC O\\n', 'O B-LOC I-LOC O O O B-PER I-PER I-PER O O O O O O O O O\\n', 'B-LOC O O O O O O O O O O O O O O O O O O O O B-LOC O\\n', 'B-LOC O O O O O O O O O O O O O O\\n', 'O O B-LOC O O O O O B-MISC I-MISC I-MISC I-MISC O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O O O O O O O O O O\\n', 'O O O O O O B-LOC O O O O O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O\\n', 'O O O O O O O O B-ORG I-ORG O O B-MISC O\\n', 'O O O O O O O O O O B-MISC O O O B-ORG I-ORG I-ORG I-ORG O B-ORG O O B-ORG I-ORG I-ORG I-ORG I-ORG I-ORG O B-ORG O O O B-ORG I-ORG I-ORG O B-LOC O O O B-ORG I-ORG\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training informations of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
