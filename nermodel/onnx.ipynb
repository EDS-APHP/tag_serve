{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.data import Data\n",
    "from ner_model import build_model, evaluate, batchify_with_label\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.onnx\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER MODEL: decoding-style loading..\n",
      "Load Model weights from file ../pretrained/baseline.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/baseline.xpt'\n",
    "path2model = '../pretrained/baseline.model'\n",
    "path2dev = '../../data/aphp_cr/dev.conll'\n",
    "decode_config_dict = {'load_model_dir':path2model, # load model file\n",
    "                      'raw_dir': path2dev\n",
    "                     }\n",
    "data = Data()\n",
    "#data.read_config(decode_config_dict)\n",
    "print(\"NER MODEL: decoding-style loading..\")\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.read_config(decode_config_dict)\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data.generate_instance('raw')\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqModel(\n",
       "  (word_hidden): WordSequence(\n",
       "    (droplstm): Dropout(p=0.4)\n",
       "    (wordrep): WordRep(\n",
       "      (char_feature): CharCNN(\n",
       "        (char_drop): Dropout(p=0.4)\n",
       "        (char_embeddings): Embedding(159, 50)\n",
       "        (char_cnn): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (drop): Dropout(p=0.4)\n",
       "      (word_embedding): Embedding(75794, 200)\n",
       "      (feature_embeddings): ModuleList()\n",
       "    )\n",
       "    (lstm): LSTM(250, 100, batch_first=True, bidirectional=True)\n",
       "    (hidden2tag): Linear(in_features=200, out_features=24, bias=True)\n",
       "  )\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "text = data.raw_Ids[:batch_size]\n",
    "batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask = \\\n",
    "batchify_with_label(text, data.HP_gpu, volatile_flag=True)\n",
    "#x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "inputs = tuple((batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "wrapPyFuncWithSymbolic(): incompatible function arguments. The following argument types are supported:\n    1. (self: torch._C.Graph, arg0: function, arg1: List[torch::jit::Value], arg2: int, arg3: function) -> iterator\n\nInvoked with: graph(%0 : Long(10, 42)\n      %1 : Long(10)\n      %2 : Long(420, 15)\n      %3 : Long(420)\n      %4 : Long(420)\n      %5 : Byte(10, 42)\n      %6 : Float(159, 50)\n      %7 : Float(50, 50, 3)\n      %8 : Float(50)\n      %9 : Float(75794, 200)\n      %10 : Float(400, 250)\n      %11 : Float(400, 100)\n      %12 : Float(400)\n      %13 : Float(400)\n      %14 : Float(400, 250)\n      %15 : Float(400, 100)\n      %16 : Float(400)\n      %17 : Float(400)\n      %18 : Float(24, 200)\n      %19 : Float(24)\n      %20 : Float(24, 24)) {\n  %21 : Long() = aten::size[dim=0](%0), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %22 : Long() = aten::size[dim=1](%0), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %23 : Float(10, 42, 200) = aten::embedding[padding_idx=-1, scale_grad_by_freq=0, sparse=0](%9, %0), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Embedding[word_embedding]\n  %24 : Long(420) = aten::_cast_Long[non_blocking=0](%3), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %25 : Long() = aten::size[dim=0](%2), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %26 : Float(420, 15, 50) = aten::embedding[padding_idx=-1, scale_grad_by_freq=0, sparse=0](%6, %2), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Embedding\n  %29 : Float(420, 15, 50) = ^Dropout(0.4, False, False)(%26), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout\n  %28 : Float(420, 15, 50) = aten::view_as(%26, %26), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout\n  %27 : Float(420, 15, 50) = aten::view[size=[420, 15, 50]](%26), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout\n  %30 : Float(420!, 50!, 15!) = aten::transpose[dim0=2, dim1=1](%29), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %31 : Float(420, 50, 15) = aten::clone(%30), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %36 : Float(420, 50, 15) = aten::_convolution[stride=[1], padding=[1], dilation=[1], transposed=0, output_padding=[0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%31, %7, %8), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %32 : Float(420, 50, 1, 15) = aten::unsqueeze[dim=2](%31), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %33 : Float(50, 50, 1, 3) = aten::unsqueeze[dim=2](%7), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %34 : Float(420, 50, 1, 15) = aten::cudnn_convolution[padding=[0, 1], stride=[1, 1], dilation=[1, 1], groups=1, benchmark=0, deterministic=0](%32, %33, %8), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %35 : Float(420, 50, 15) = aten::squeeze[dim=2](%34), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %37 : Long() = aten::size[dim=2](%36), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %45 : Float(420, 50, 1), %46 : Long(420, 50, 1) = aten::max_pool1d_with_indices[kernel_size=[15], stride=[], padding=[0], dilation=[1], ceil_mode=0](%36), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %38 : Float(420, 50, 1, 15) = aten::unsqueeze[dim=2](%36), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %41 : Float(420, 50, 1, 1), %42 : Long(420, 50, 1, 1) = aten::max_pool2d_with_indices[kernel_size=[1, 15], stride=[1, 15], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%38), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %39 : Float(420, 50, 1, 1), %40 : Long(420, 50, 1, 1) = aten::max_pool2d_with_indices_forward[kernel_size=[1, 15], stride=[1, 15], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%38), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %43 : Long(420, 50, 1) = aten::squeeze[dim=2](%42), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %44 : Float(420, 50, 1) = aten::squeeze[dim=2](%41), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %47 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %48 : Dynamic = aten::stack[dim=0](%25, %47), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %49 : Float(420, 50) = aten::view(%45, %48), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %50 : Long(420) = aten::_cast_Long[non_blocking=0](%4), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %60 : Float(420, 50) = aten::index(%49, %50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %51 : Long() = aten::max(%50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %52 : Long() = aten::min(%50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %53 : Long(420) = aten::remainder[other={420}](%50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %54 : Long(420) = aten::mul[other={50}](%53), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %55 : Long(420, 1) = aten::view[size=[420, 1]](%54), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %56 : Long(420!, 50!) = aten::expand[size=[420, 50], implicit=1](%55), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %57 : Long(420!, 50) = prim::Constant[value=<Tensor>](), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %58 : Long(420, 50) = aten::add[alpha={1}](%56, %57), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %59 : Float(420, 50) = aten::take(%49, %58), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %61 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %62 : Dynamic = aten::stack[dim=0](%21, %22, %61), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %63 : Float(10, 42, 50) = aten::view(%60, %62), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %64 : Float(10, 42, 250) = aten::cat[dim=2](%23, %63), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %67 : Float(10, 42, 250) = ^Dropout(0.4, False, False)(%64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n  %66 : Float(10, 42, 250) = aten::view_as(%64, %64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n  %65 : Float(10, 42, 250) = aten::view[size=[10, 42, 250]](%64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n  %173 : Float(202, 250), %174 : Long(42) = ^PackPadded([42 30 26 25 23 21 18  9  6  2], True)(%67), scope: SeqModel/WordSequence[word_hidden]\n  %68 : Float(42!, 10!, 250) = aten::transpose[dim0=0, dim1=1](%67), scope: SeqModel/WordSequence[word_hidden]\n  %69 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %70 : Byte() = aten::ne[other={10}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %71 : Long() = aten::sub[other={0}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %72 : Float(2!, 10!, 250) = aten::slice[dim=0, start=0, end=2, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %73 : Float(2, 10, 250) = aten::clone(%72), scope: SeqModel/WordSequence[word_hidden]\n  %74 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %75 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %76 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %77 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %78 : Dynamic = aten::stack[dim=0](%77, %76), scope: SeqModel/WordSequence[word_hidden]\n  %79 : Float(20, 250) = aten::view(%73, %78), scope: SeqModel/WordSequence[word_hidden]\n  %80 : Long() = aten::sub[other={1}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %81 : Float(4!, 10!, 250) = aten::slice[dim=0, start=2, end=6, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %82 : Float(4!, 9!, 250) = aten::slice[dim=1, start=0, end=9, step=1](%81), scope: SeqModel/WordSequence[word_hidden]\n  %83 : Float(4, 9, 250) = aten::clone(%82), scope: SeqModel/WordSequence[word_hidden]\n  %84 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %85 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %86 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %87 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %88 : Dynamic = aten::stack[dim=0](%87, %86), scope: SeqModel/WordSequence[word_hidden]\n  %89 : Float(36, 250) = aten::view(%83, %88), scope: SeqModel/WordSequence[word_hidden]\n  %90 : Long() = aten::sub[other={2}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %91 : Float(3!, 10!, 250) = aten::slice[dim=0, start=6, end=9, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %92 : Float(3!, 8!, 250) = aten::slice[dim=1, start=0, end=8, step=1](%91), scope: SeqModel/WordSequence[word_hidden]\n  %93 : Float(3, 8, 250) = aten::clone(%92), scope: SeqModel/WordSequence[word_hidden]\n  %94 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %95 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %96 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %97 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %98 : Dynamic = aten::stack[dim=0](%97, %96), scope: SeqModel/WordSequence[word_hidden]\n  %99 : Float(24, 250) = aten::view(%93, %98), scope: SeqModel/WordSequence[word_hidden]\n  %100 : Long() = aten::sub[other={3}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %101 : Float(9!, 10!, 250) = aten::slice[dim=0, start=9, end=18, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %102 : Float(9!, 7!, 250) = aten::slice[dim=1, start=0, end=7, step=1](%101), scope: SeqModel/WordSequence[word_hidden]\n  %103 : Float(9, 7, 250) = aten::clone(%102), scope: SeqModel/WordSequence[word_hidden]\n  %104 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %105 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %106 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %107 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %108 : Dynamic = aten::stack[dim=0](%107, %106), scope: SeqModel/WordSequence[word_hidden]\n  %109 : Float(63, 250) = aten::view(%103, %108), scope: SeqModel/WordSequence[word_hidden]\n  %110 : Long() = aten::sub[other={4}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %111 : Float(3!, 10!, 250) = aten::slice[dim=0, start=18, end=21, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %112 : Float(3!, 6!, 250) = aten::slice[dim=1, start=0, end=6, step=1](%111), scope: SeqModel/WordSequence[word_hidden]\n  %113 : Float(3, 6, 250) = aten::clone(%112), scope: SeqModel/WordSequence[word_hidden]\n  %114 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %115 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %116 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %117 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %118 : Dynamic = aten::stack[dim=0](%117, %116), scope: SeqModel/WordSequence[word_hidden]\n  %119 : Float(18, 250) = aten::view(%113, %118), scope: SeqModel/WordSequence[word_hidden]\n  %120 : Long() = aten::sub[other={5}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %121 : Float(2!, 10!, 250) = aten::slice[dim=0, start=21, end=23, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %122 : Float(2!, 5!, 250) = aten::slice[dim=1, start=0, end=5, step=1](%121), scope: SeqModel/WordSequence[word_hidden]\n  %123 : Float(2, 5, 250) = aten::clone(%122), scope: SeqModel/WordSequence[word_hidden]\n  %124 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %125 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %126 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %127 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %128 : Dynamic = aten::stack[dim=0](%127, %126), scope: SeqModel/WordSequence[word_hidden]\n  %129 : Float(10, 250) = aten::view(%123, %128), scope: SeqModel/WordSequence[word_hidden]\n  %130 : Long() = aten::sub[other={6}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %131 : Float(2!, 10!, 250) = aten::slice[dim=0, start=23, end=25, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %132 : Float(2!, 4!, 250) = aten::slice[dim=1, start=0, end=4, step=1](%131), scope: SeqModel/WordSequence[word_hidden]\n  %133 : Float(2, 4, 250) = aten::clone(%132), scope: SeqModel/WordSequence[word_hidden]\n  %134 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %135 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %136 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %137 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %138 : Dynamic = aten::stack[dim=0](%137, %136), scope: SeqModel/WordSequence[word_hidden]\n  %139 : Float(8, 250) = aten::view(%133, %138), scope: SeqModel/WordSequence[word_hidden]\n  %140 : Long() = aten::sub[other={7}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %141 : Float(1!, 10!, 250) = aten::slice[dim=0, start=25, end=26, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %142 : Float(1!, 3!, 250) = aten::slice[dim=1, start=0, end=3, step=1](%141), scope: SeqModel/WordSequence[word_hidden]\n  %143 : Float(1, 3, 250) = aten::clone(%142), scope: SeqModel/WordSequence[word_hidden]\n  %144 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %145 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %146 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %147 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %148 : Dynamic = aten::stack[dim=0](%147, %146), scope: SeqModel/WordSequence[word_hidden]\n  %149 : Float(3, 250) = aten::view(%143, %148), scope: SeqModel/WordSequence[word_hidden]\n  %150 : Long() = aten::sub[other={8}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %151 : Float(4!, 10!, 250) = aten::slice[dim=0, start=26, end=30, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %152 : Float(4!, 2!, 250) = aten::slice[dim=1, start=0, end=2, step=1](%151), scope: SeqModel/WordSequence[word_hidden]\n  %153 : Float(4, 2, 250) = aten::clone(%152), scope: SeqModel/WordSequence[word_hidden]\n  %154 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %155 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %156 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %157 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %158 : Dynamic = aten::stack[dim=0](%157, %156), scope: SeqModel/WordSequence[word_hidden]\n  %159 : Float(8, 250) = aten::view(%153, %158), scope: SeqModel/WordSequence[word_hidden]\n  %160 : Long() = aten::sub[other={9}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %161 : Float(12!, 10!, 250) = aten::slice[dim=0, start=30, end=42, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %162 : Float(12!, 1!, 250) = aten::slice[dim=1, start=0, end=1, step=1](%161), scope: SeqModel/WordSequence[word_hidden]\n  %163 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %164 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %165 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %166 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %167 : Dynamic = aten::stack[dim=0](%166, %165), scope: SeqModel/WordSequence[word_hidden]\n  %168 : Float(12, 250) = aten::view(%162, %167), scope: SeqModel/WordSequence[word_hidden]\n  %169 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %170 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %171 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %172 : Float(202, 250) = aten::cat[dim=0](%79, %89, %99, %109, %119, %129, %139, %149, %159, %168), scope: SeqModel/WordSequence[word_hidden]\n  return ();\n}\n, <function _symbolic_pack_padded_sequence.<locals>.pack_padded_sequence_trace_wrapper at 0x7f326ccf5378>, [67 defined in (%67 : Float(10, 42, 250) = ^Dropout(0.4, False, False)(%64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n), array([42, 30, 26, 25, 23, 21, 18,  9,  6,  2])], 2, <function _symbolic_pack_padded_sequence.<locals>._onnx_symbolic_pack_padded_sequence at 0x7f326ccf52f0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-eed5084168bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m# model input (or a tuple for multiple inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0;34m\"super_resolution.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# where to save the model (can be a file or file-like object)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                export_params=True)  \n\u001b[0m",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate)\u001b[0m\n\u001b[1;32m    224\u001b[0m                                                \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                                                \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                                                example_outputs, propagate)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# TODO: Don't allocate a in-memory string for the protobuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, f, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\'forward\\' method must be a script method'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args, training)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# training mode was.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morig_state_dict_keys\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mget_trace_graph\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLegacyTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtrace_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_trace_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mout_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_tracing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/8to2/matthieu/myTagger/tag_serve/nermodel/model/seqmodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_inputs, features_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, mask)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_seq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_seq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_seq_recover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         outs = self.word_hidden(\n\u001b[0;32m---> 60\u001b[0;31m             word_inputs, features_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traced_module_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/8to2/matthieu/myTagger/tag_serve/nermodel/model/wordsequence.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                         \u001b[0mpacked_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_represent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_seq_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msymbolic_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0moutput_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymbolic_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msymbolic_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         for var, val in zip(\n",
      "\u001b[0;32m/home/matthieu/anaconda3/lib/python3.5/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36m_symbolic_pack_padded_sequence\u001b[0;34m(g, input, lengths, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    165\u001b[0m     outputs = g.wrapPyFuncWithSymbolic(\n\u001b[1;32m    166\u001b[0m         \u001b[0mpack_padded_sequence_trace_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         _onnx_symbolic_pack_padded_sequence)\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: wrapPyFuncWithSymbolic(): incompatible function arguments. The following argument types are supported:\n    1. (self: torch._C.Graph, arg0: function, arg1: List[torch::jit::Value], arg2: int, arg3: function) -> iterator\n\nInvoked with: graph(%0 : Long(10, 42)\n      %1 : Long(10)\n      %2 : Long(420, 15)\n      %3 : Long(420)\n      %4 : Long(420)\n      %5 : Byte(10, 42)\n      %6 : Float(159, 50)\n      %7 : Float(50, 50, 3)\n      %8 : Float(50)\n      %9 : Float(75794, 200)\n      %10 : Float(400, 250)\n      %11 : Float(400, 100)\n      %12 : Float(400)\n      %13 : Float(400)\n      %14 : Float(400, 250)\n      %15 : Float(400, 100)\n      %16 : Float(400)\n      %17 : Float(400)\n      %18 : Float(24, 200)\n      %19 : Float(24)\n      %20 : Float(24, 24)) {\n  %21 : Long() = aten::size[dim=0](%0), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %22 : Long() = aten::size[dim=1](%0), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %23 : Float(10, 42, 200) = aten::embedding[padding_idx=-1, scale_grad_by_freq=0, sparse=0](%9, %0), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Embedding[word_embedding]\n  %24 : Long(420) = aten::_cast_Long[non_blocking=0](%3), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %25 : Long() = aten::size[dim=0](%2), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %26 : Float(420, 15, 50) = aten::embedding[padding_idx=-1, scale_grad_by_freq=0, sparse=0](%6, %2), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Embedding\n  %29 : Float(420, 15, 50) = ^Dropout(0.4, False, False)(%26), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout\n  %28 : Float(420, 15, 50) = aten::view_as(%26, %26), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout\n  %27 : Float(420, 15, 50) = aten::view[size=[420, 15, 50]](%26), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout\n  %30 : Float(420!, 50!, 15!) = aten::transpose[dim0=2, dim1=1](%29), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %31 : Float(420, 50, 15) = aten::clone(%30), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %36 : Float(420, 50, 15) = aten::_convolution[stride=[1], padding=[1], dilation=[1], transposed=0, output_padding=[0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%31, %7, %8), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %32 : Float(420, 50, 1, 15) = aten::unsqueeze[dim=2](%31), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %33 : Float(50, 50, 1, 3) = aten::unsqueeze[dim=2](%7), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %34 : Float(420, 50, 1, 15) = aten::cudnn_convolution[padding=[0, 1], stride=[1, 1], dilation=[1, 1], groups=1, benchmark=0, deterministic=0](%32, %33, %8), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %35 : Float(420, 50, 15) = aten::squeeze[dim=2](%34), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Conv1d\n  %37 : Long() = aten::size[dim=2](%36), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %45 : Float(420, 50, 1), %46 : Long(420, 50, 1) = aten::max_pool1d_with_indices[kernel_size=[15], stride=[], padding=[0], dilation=[1], ceil_mode=0](%36), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %38 : Float(420, 50, 1, 15) = aten::unsqueeze[dim=2](%36), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %41 : Float(420, 50, 1, 1), %42 : Long(420, 50, 1, 1) = aten::max_pool2d_with_indices[kernel_size=[1, 15], stride=[1, 15], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%38), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %39 : Float(420, 50, 1, 1), %40 : Long(420, 50, 1, 1) = aten::max_pool2d_with_indices_forward[kernel_size=[1, 15], stride=[1, 15], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%38), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %43 : Long(420, 50, 1) = aten::squeeze[dim=2](%42), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %44 : Float(420, 50, 1) = aten::squeeze[dim=2](%41), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %47 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %48 : Dynamic = aten::stack[dim=0](%25, %47), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %49 : Float(420, 50) = aten::view(%45, %48), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %50 : Long(420) = aten::_cast_Long[non_blocking=0](%4), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %60 : Float(420, 50) = aten::index(%49, %50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %51 : Long() = aten::max(%50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %52 : Long() = aten::min(%50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %53 : Long(420) = aten::remainder[other={420}](%50), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %54 : Long(420) = aten::mul[other={50}](%53), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %55 : Long(420, 1) = aten::view[size=[420, 1]](%54), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %56 : Long(420!, 50!) = aten::expand[size=[420, 50], implicit=1](%55), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %57 : Long(420!, 50) = prim::Constant[value=<Tensor>](), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %58 : Long(420, 50) = aten::add[alpha={1}](%56, %57), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %59 : Float(420, 50) = aten::take(%49, %58), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %61 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %62 : Dynamic = aten::stack[dim=0](%21, %22, %61), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %63 : Float(10, 42, 50) = aten::view(%60, %62), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %64 : Float(10, 42, 250) = aten::cat[dim=2](%23, %63), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]\n  %67 : Float(10, 42, 250) = ^Dropout(0.4, False, False)(%64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n  %66 : Float(10, 42, 250) = aten::view_as(%64, %64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n  %65 : Float(10, 42, 250) = aten::view[size=[10, 42, 250]](%64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n  %173 : Float(202, 250), %174 : Long(42) = ^PackPadded([42 30 26 25 23 21 18  9  6  2], True)(%67), scope: SeqModel/WordSequence[word_hidden]\n  %68 : Float(42!, 10!, 250) = aten::transpose[dim0=0, dim1=1](%67), scope: SeqModel/WordSequence[word_hidden]\n  %69 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %70 : Byte() = aten::ne[other={10}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %71 : Long() = aten::sub[other={0}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %72 : Float(2!, 10!, 250) = aten::slice[dim=0, start=0, end=2, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %73 : Float(2, 10, 250) = aten::clone(%72), scope: SeqModel/WordSequence[word_hidden]\n  %74 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %75 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %76 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %77 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %78 : Dynamic = aten::stack[dim=0](%77, %76), scope: SeqModel/WordSequence[word_hidden]\n  %79 : Float(20, 250) = aten::view(%73, %78), scope: SeqModel/WordSequence[word_hidden]\n  %80 : Long() = aten::sub[other={1}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %81 : Float(4!, 10!, 250) = aten::slice[dim=0, start=2, end=6, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %82 : Float(4!, 9!, 250) = aten::slice[dim=1, start=0, end=9, step=1](%81), scope: SeqModel/WordSequence[word_hidden]\n  %83 : Float(4, 9, 250) = aten::clone(%82), scope: SeqModel/WordSequence[word_hidden]\n  %84 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %85 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %86 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %87 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %88 : Dynamic = aten::stack[dim=0](%87, %86), scope: SeqModel/WordSequence[word_hidden]\n  %89 : Float(36, 250) = aten::view(%83, %88), scope: SeqModel/WordSequence[word_hidden]\n  %90 : Long() = aten::sub[other={2}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %91 : Float(3!, 10!, 250) = aten::slice[dim=0, start=6, end=9, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %92 : Float(3!, 8!, 250) = aten::slice[dim=1, start=0, end=8, step=1](%91), scope: SeqModel/WordSequence[word_hidden]\n  %93 : Float(3, 8, 250) = aten::clone(%92), scope: SeqModel/WordSequence[word_hidden]\n  %94 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %95 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %96 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %97 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %98 : Dynamic = aten::stack[dim=0](%97, %96), scope: SeqModel/WordSequence[word_hidden]\n  %99 : Float(24, 250) = aten::view(%93, %98), scope: SeqModel/WordSequence[word_hidden]\n  %100 : Long() = aten::sub[other={3}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %101 : Float(9!, 10!, 250) = aten::slice[dim=0, start=9, end=18, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %102 : Float(9!, 7!, 250) = aten::slice[dim=1, start=0, end=7, step=1](%101), scope: SeqModel/WordSequence[word_hidden]\n  %103 : Float(9, 7, 250) = aten::clone(%102), scope: SeqModel/WordSequence[word_hidden]\n  %104 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %105 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %106 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %107 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %108 : Dynamic = aten::stack[dim=0](%107, %106), scope: SeqModel/WordSequence[word_hidden]\n  %109 : Float(63, 250) = aten::view(%103, %108), scope: SeqModel/WordSequence[word_hidden]\n  %110 : Long() = aten::sub[other={4}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %111 : Float(3!, 10!, 250) = aten::slice[dim=0, start=18, end=21, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %112 : Float(3!, 6!, 250) = aten::slice[dim=1, start=0, end=6, step=1](%111), scope: SeqModel/WordSequence[word_hidden]\n  %113 : Float(3, 6, 250) = aten::clone(%112), scope: SeqModel/WordSequence[word_hidden]\n  %114 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %115 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %116 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %117 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %118 : Dynamic = aten::stack[dim=0](%117, %116), scope: SeqModel/WordSequence[word_hidden]\n  %119 : Float(18, 250) = aten::view(%113, %118), scope: SeqModel/WordSequence[word_hidden]\n  %120 : Long() = aten::sub[other={5}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %121 : Float(2!, 10!, 250) = aten::slice[dim=0, start=21, end=23, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %122 : Float(2!, 5!, 250) = aten::slice[dim=1, start=0, end=5, step=1](%121), scope: SeqModel/WordSequence[word_hidden]\n  %123 : Float(2, 5, 250) = aten::clone(%122), scope: SeqModel/WordSequence[word_hidden]\n  %124 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %125 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %126 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %127 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %128 : Dynamic = aten::stack[dim=0](%127, %126), scope: SeqModel/WordSequence[word_hidden]\n  %129 : Float(10, 250) = aten::view(%123, %128), scope: SeqModel/WordSequence[word_hidden]\n  %130 : Long() = aten::sub[other={6}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %131 : Float(2!, 10!, 250) = aten::slice[dim=0, start=23, end=25, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %132 : Float(2!, 4!, 250) = aten::slice[dim=1, start=0, end=4, step=1](%131), scope: SeqModel/WordSequence[word_hidden]\n  %133 : Float(2, 4, 250) = aten::clone(%132), scope: SeqModel/WordSequence[word_hidden]\n  %134 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %135 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %136 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %137 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %138 : Dynamic = aten::stack[dim=0](%137, %136), scope: SeqModel/WordSequence[word_hidden]\n  %139 : Float(8, 250) = aten::view(%133, %138), scope: SeqModel/WordSequence[word_hidden]\n  %140 : Long() = aten::sub[other={7}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %141 : Float(1!, 10!, 250) = aten::slice[dim=0, start=25, end=26, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %142 : Float(1!, 3!, 250) = aten::slice[dim=1, start=0, end=3, step=1](%141), scope: SeqModel/WordSequence[word_hidden]\n  %143 : Float(1, 3, 250) = aten::clone(%142), scope: SeqModel/WordSequence[word_hidden]\n  %144 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %145 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %146 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %147 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %148 : Dynamic = aten::stack[dim=0](%147, %146), scope: SeqModel/WordSequence[word_hidden]\n  %149 : Float(3, 250) = aten::view(%143, %148), scope: SeqModel/WordSequence[word_hidden]\n  %150 : Long() = aten::sub[other={8}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %151 : Float(4!, 10!, 250) = aten::slice[dim=0, start=26, end=30, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %152 : Float(4!, 2!, 250) = aten::slice[dim=1, start=0, end=2, step=1](%151), scope: SeqModel/WordSequence[word_hidden]\n  %153 : Float(4, 2, 250) = aten::clone(%152), scope: SeqModel/WordSequence[word_hidden]\n  %154 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %155 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %156 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %157 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %158 : Dynamic = aten::stack[dim=0](%157, %156), scope: SeqModel/WordSequence[word_hidden]\n  %159 : Float(8, 250) = aten::view(%153, %158), scope: SeqModel/WordSequence[word_hidden]\n  %160 : Long() = aten::sub[other={9}, alpha={1}](%69), scope: SeqModel/WordSequence[word_hidden]\n  %161 : Float(12!, 10!, 250) = aten::slice[dim=0, start=30, end=42, step=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %162 : Float(12!, 1!, 250) = aten::slice[dim=1, start=0, end=1, step=1](%161), scope: SeqModel/WordSequence[word_hidden]\n  %163 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %164 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %165 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %166 : Long() = prim::Constant[value={-1}](), scope: SeqModel/WordSequence[word_hidden]\n  %167 : Dynamic = aten::stack[dim=0](%166, %165), scope: SeqModel/WordSequence[word_hidden]\n  %168 : Float(12, 250) = aten::view(%162, %167), scope: SeqModel/WordSequence[word_hidden]\n  %169 : Long() = aten::size[dim=0](%68), scope: SeqModel/WordSequence[word_hidden]\n  %170 : Long() = aten::size[dim=1](%68), scope: SeqModel/WordSequence[word_hidden]\n  %171 : Long() = aten::size[dim=2](%68), scope: SeqModel/WordSequence[word_hidden]\n  %172 : Float(202, 250) = aten::cat[dim=0](%79, %89, %99, %109, %119, %129, %139, %149, %159, %168), scope: SeqModel/WordSequence[word_hidden]\n  return ();\n}\n, <function _symbolic_pack_padded_sequence.<locals>.pack_padded_sequence_trace_wrapper at 0x7f326ccf5378>, [67 defined in (%67 : Float(10, 42, 250) = ^Dropout(0.4, False, False)(%64), scope: SeqModel/WordSequence[word_hidden]/WordRep[wordrep]/Dropout[drop]\n), array([42, 30, 26, 25, 23, 21, 18,  9,  6,  2])], 2, <function _symbolic_pack_padded_sequence.<locals>._onnx_symbolic_pack_padded_sequence at 0x7f326ccf52f0>"
     ]
    }
   ],
   "source": [
    "torch_out = torch.onnx._export(model,             # model being run\n",
    "                               inputs,                       # model input (or a tuple for multiple inputs)\n",
    "                               \"super_resolution.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                               export_params=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
