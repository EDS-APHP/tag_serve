{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "import spacy.displacy as  dp\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 'salut les aminches. Comment ça va ?'\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import myTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = myTokenizer('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2text = '/home/matthieu/Documents/aphp/rapport/fic_CR.txt'\n",
    "path2ann = '/home/matthieu/Documents/aphp/rapport/fic_CR.ann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path2ann, 'r') as f:\n",
    "    labels = [l.split('\\t')[1].split(' ') for l in f.read().splitlines()]\n",
    "with open(path2text, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Serving on port 5000...\u001b[0m\n",
      "    Using the 'ent' visualizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Aug/2018 13:13:52] \"GET / HTTP/1.1\" 200 9499\n",
      "127.0.0.1 - - [12/Aug/2018 13:13:52] \"GET /favicon.ico HTTP/1.1\" 200 9499\n",
      "127.0.0.1 - - [12/Aug/2018 13:13:52] \"GET /favicon.ico HTTP/1.1\" 200 9499\n"
     ]
    }
   ],
   "source": [
    "entities = [{'start':int(l[1]), 'end':int(l[2]), 'label':l[0]} for l in labels]\n",
    "visu = [{'text':text, 'ents':entities, 'title':None}]\n",
    "dp.serve(visu, style='ent', manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(data):\n",
    "    ''' align a dictionnary of sequences \n",
    "        input:\n",
    "            data, dict of sequences { 'key1': ['I', 'dream', 'of', 'the', 'Moon'] \n",
    "                        'key2': [O, O, O, O, 'B-LOC']}\n",
    "        output:\n",
    "\n",
    "            dict of strings {'key1': ['I dream of the Moon'] \n",
    "                             'key2': ['O O     O  O   B-LOC']}\n",
    "    '''\n",
    "    spacings = [max([len(seq[i]) for seq in data.values()]) for i in range(len(data[list(data.keys())[0]]))]\n",
    "\n",
    "    data_aligned = dict()\n",
    "\n",
    "    for key, seq in data.items():\n",
    "        str_aligned = \"\"\n",
    "        for token, spacing in zip(seq, spacings):\n",
    "            str_aligned += token + \" \"*(spacing-len(token) + 1)\n",
    "        data_aligned[key] = str_aligned\n",
    "    return data_aligned\n",
    "#print(align_data(align)['inputs'])\n",
    "#print(align_data(align)['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER MODEL: decoding-style loading..\n",
      "Load Model weights from file ../pretrained/baseline.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/baseline.xpt'\n",
    "path2model = '../pretrained/baseline.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "#data.read_config(decode_config_dict)\n",
    "print(\"NER MODEL: decoding-style loading..\")\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.read_config(decode_config_dict)\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en', disable=['tagger','ner', 'parser'])\n",
    "tokenizer.add_pipe(tokenizer.create_pipe('sentencizer')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'reading', 'Camus', '.']\n",
      "['I', 'love', 'you', 'Juliette', '.']\n",
      "['I', 'love', 'reading', 'Camus', '.', '', 'I', 'love', 'you', 'Juliette', '.', '']\n",
      "Processing time 0.0049 s\n",
      "Decoding speed: 437.96 st/s\n",
      "I love reading Camus . I love you Juliette . \n",
      "O O    O       B-PER O O O    O   B-PER    O \n"
     ]
    }
   ],
   "source": [
    "input_data = 'I love reading Camus.'\n",
    "text = tokenizer(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in text.sents:\n",
    "    sentence = [token.string.strip() for token in sent]\n",
    "    print(sentence)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += sentence\n",
    "    input_model += sentence + ['']\n",
    "\n",
    "print(input_model)\n",
    "\n",
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.2f} st/s'.format(speed))\n",
    "# reconstruct a unique sequence for the client\n",
    "live = True\n",
    "if live: \n",
    "    output_client = []\n",
    "    for l in pred_results:\n",
    "        output_client += l\n",
    "    output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "\n",
    "    print(output_aligned['raw_input'])\n",
    "    print(output_aligned['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2txt = out_folder + os.path.splitext(file_name)[0] + '.txt'\n",
    "path2ann = out_folder + os.path.splitext(file_name)[0] + '.ann'\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()\n",
    "\n",
    "text = tokenizer(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in text.sents:\n",
    "    sentence = [token.string.strip() for token in sent]\n",
    "    #print(sentence)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += sentence\n",
    "    input_model += sentence + ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.11 s\n",
      "Decoding speed: 179.36 st/s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.2f} st/s'.format(speed))\n",
    "# reconstruct a unique sequence for the client\n",
    "live = True\n",
    "output_client = pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T1\\tLOC 0 6\\tFrance\\n',\n",
       " 'T1\\tMISC 8 14\\tFrench\\n',\n",
       " 'T1\\tLOC 41 47\\tFrench\\n',\n",
       " 'T1\\tLOC 48 56\\tRepublic\\n',\n",
       " 'T1\\tMISC 58 64\\tFrench\\n',\n",
       " 'T1\\tMISC 64 65\\t:\\n',\n",
       " 'T1\\tMISC 66 76\\tRépublique\\n',\n",
       " 'T1\\tLOC 170 176\\tFrance\\n',\n",
       " 'T1\\tLOC 180 187\\tWestern\\n',\n",
       " 'T1\\tLOC 188 194\\tEurope\\n',\n",
       " 'T1\\tORG 236 253\\tterritories.[XIII\\n',\n",
       " 'T1\\tLOC 280 286\\tFrance\\n',\n",
       " 'T1\\tLOC 304 317\\tMediterranean\\n',\n",
       " 'T1\\tLOC 318 321\\tSea\\n',\n",
       " 'T1\\tORG 329 336\\tEnglish\\n',\n",
       " 'T1\\tORG 337 344\\tChannel\\n',\n",
       " 'T1\\tLOC 353 358\\tNorth\\n',\n",
       " 'T1\\tLOC 359 362\\tSea\\n',\n",
       " 'T1\\tLOC 377 382\\tRhine\\n',\n",
       " 'T1\\tLOC 390 398\\tAtlantic\\n',\n",
       " 'T1\\tLOC 399 404\\tOcean\\n',\n",
       " 'T1\\tLOC 439 445\\tFrench\\n',\n",
       " 'T1\\tLOC 446 452\\tGuiana\\n',\n",
       " 'T1\\tLOC 456 461\\tSouth\\n',\n",
       " 'T1\\tLOC 462 469\\tAmerica\\n',\n",
       " 'T1\\tLOC 497 505\\tAtlantic\\n',\n",
       " 'T1\\tLOC 507 514\\tPacific\\n',\n",
       " 'T1\\tMISC 519 525\\tIndian\\n',\n",
       " 'T1\\tLOC 735 741\\tFrance\\n',\n",
       " 'T1\\tLOC 802 807\\tParis\\n',\n",
       " 'T1\\tORG 911 920\\tMarseille\\n',\n",
       " 'T1\\tORG 922 926\\tLyon\\n',\n",
       " 'T1\\tORG 928 933\\tLille\\n',\n",
       " 'T1\\tORG 935 939\\tNice\\n',\n",
       " 'T1\\tPER 941 949\\tToulouse\\n',\n",
       " 'T1\\tORG 954 964\\tStrasbourg\\n',\n",
       " 'T1\\tLOC 974 977\\tthe\\n',\n",
       " 'T1\\tLOC 978 982\\tIron\\n',\n",
       " 'T1\\tLOC 1000 1012\\tmetropolitan\\n',\n",
       " 'T1\\tPER 1037 1040\\tthe\\n',\n",
       " 'T1\\tORG 1048 1049\\ta\\n',\n",
       " 'T1\\tLOC 1065 1069\\tRome\\n',\n",
       " 'T1\\tMISC 1093 1095\\tBC\\n',\n",
       " 'T1\\tPER 1129 1137\\tGermanic\\n',\n",
       " 'T1\\tPER 1138 1144\\tFranks\\n',\n",
       " 'T1\\tLOC 1168 1175\\tKingdom\\n',\n",
       " 'T1\\tLOC 1179 1185\\tFrance\\n',\n",
       " 'T1\\tLOC 1187 1193\\tFrance\\n',\n",
       " 'T1\\tMISC 1213 1221\\tEuropean\\n',\n",
       " 'T1\\tLOC 1235 1239\\tLate\\n',\n",
       " 'T1\\tLOC 1240 1246\\tMiddle\\n',\n",
       " 'T1\\tLOC 1247 1251\\tAges\\n',\n",
       " 'T1\\tMISC 1281 1288\\tHundred\\n',\n",
       " 'T1\\tMISC 1289 1294\\tYears\\n',\n",
       " \"T1\\tMISC 1294 1295\\t'\\n\",\n",
       " 'T1\\tMISC 1296 1299\\tWar\\n',\n",
       " 'T1\\tPER 1327 1338\\tRenaissance\\n',\n",
       " 'T1\\tMISC 1340 1346\\tFrench\\n',\n",
       " 'T1\\tMISC 1547 1556\\tCatholics\\n',\n",
       " 'T1\\tPER 1561 1572\\tProtestants\\n',\n",
       " 'T1\\tLOC 1574 1583\\tHuguenots\\n',\n",
       " 'T1\\tLOC 1586 1592\\tFrance\\n',\n",
       " 'T1\\tLOC 1600 1606\\tEurope\\n',\n",
       " 'T1\\tPER 1664 1669\\tLouis\\n',\n",
       " 'T1\\tPER 1670 1677\\tXIV.[12\\n',\n",
       " 'T1\\tMISC 1709 1715\\tFrench\\n',\n",
       " 'T1\\tMISC 1716 1726\\tRevolution\\n',\n",
       " 'T1\\tORG 1844 1855\\tDeclaration\\n',\n",
       " 'T1\\tORG 1856 1858\\tof\\n',\n",
       " 'T1\\tORG 1859 1862\\tthe\\n',\n",
       " 'T1\\tORG 1863 1869\\tRights\\n',\n",
       " 'T1\\tORG 1870 1872\\tof\\n',\n",
       " 'T1\\tORG 1873 1876\\tMan\\n',\n",
       " 'T1\\tORG 1888 1895\\tCitizen\\n',\n",
       " 'T1\\tPER 1959 1966\\tcentury\\n',\n",
       " 'T1\\tMISC 2003 2006\\tthe\\n',\n",
       " 'T1\\tMISC 2007 2012\\tFirst\\n',\n",
       " 'T1\\tMISC 2013 2019\\tFrench\\n',\n",
       " 'T1\\tMISC 2043 2053\\tNapoleonic\\n',\n",
       " 'T1\\tMISC 2054 2058\\tWars\\n',\n",
       " 'T1\\tLOC 2092 2098\\tEurope\\n',\n",
       " 'T1\\tLOC 2130 2136\\tEmpire\\n',\n",
       " 'T1\\tLOC 2138 2144\\tFrance\\n',\n",
       " 'T1\\tMISC 2234 2240\\tFrench\\n',\n",
       " 'T1\\tMISC 2241 2246\\tThird\\n',\n",
       " 'T1\\tLOC 2247 2255\\tRepublic\\n',\n",
       " 'T1\\tLOC 2265 2271\\tFrance\\n',\n",
       " 'T1\\tMISC 2299 2304\\tWorld\\n',\n",
       " 'T1\\tMISC 2305 2308\\tWar\\n',\n",
       " 'T1\\tMISC 2365 2371\\tAllied\\n',\n",
       " 'T1\\tMISC 2372 2378\\tPowers\\n',\n",
       " 'T1\\tMISC 2379 2381\\tin\\n',\n",
       " 'T1\\tMISC 2382 2387\\tWorld\\n',\n",
       " 'T1\\tMISC 2388 2391\\tWar\\n',\n",
       " 'T1\\tMISC 2392 2394\\tII\\n',\n",
       " 'T1\\tORG 2429 2433\\tAxis\\n',\n",
       " 'T1\\tLOC 2482 2488\\tFourth\\n',\n",
       " 'T1\\tLOC 2489 2497\\tRepublic\\n',\n",
       " 'T1\\tMISC 2555 2563\\tAlgerian\\n',\n",
       " 'T1\\tMISC 2564 2567\\tWar\\n',\n",
       " 'T1\\tLOC 2573 2578\\tFifth\\n',\n",
       " 'T1\\tLOC 2579 2587\\tRepublic\\n',\n",
       " 'T1\\tPER 2596 2603\\tCharles\\n',\n",
       " 'T1\\tPER 2604 2606\\tde\\n',\n",
       " 'T1\\tPER 2607 2613\\tGaulle\\n',\n",
       " 'T1\\tLOC 2653 2660\\tAlgeria\\n',\n",
       " 'T1\\tLOC 2795 2801\\tFrance\\n',\n",
       " 'T1\\tLOC 2802 2802\\t\\n',\n",
       " 'T1\\tLOC 2883 2889\\tEurope\\n',\n",
       " 'T1\\tMISC 2925 2931\\tUNESCO\\n',\n",
       " 'T1\\tMISC 2932 2937\\tWorld\\n',\n",
       " 'T1\\tMISC 2938 2946\\tHeritage\\n',\n",
       " 'T1\\tMISC 2947 2952\\tSites\\n',\n",
       " 'T1\\tLOC 3044 3050\\tFrance\\n',\n",
       " 'T1\\tLOC 3259 3265\\tFrance\\n',\n",
       " 'T1\\tLOC 3381 3387\\tFrance\\n',\n",
       " 'T1\\tORG 3490 3496\\tUnited\\n',\n",
       " 'T1\\tORG 3497 3504\\tNations\\n',\n",
       " 'T1\\tORG 3505 3513\\tSecurity\\n',\n",
       " 'T1\\tORG 3514 3521\\tCouncil\\n',\n",
       " 'T1\\tORG 3622 3630\\tEuropean\\n',\n",
       " 'T1\\tORG 3631 3636\\tUnion\\n',\n",
       " 'T1\\tMISC 3645 3657\\tEurozone.[20\\n',\n",
       " 'T1\\tORG 3686 3691\\tGroup\\n',\n",
       " 'T1\\tORG 3692 3694\\tof\\n',\n",
       " 'T1\\tORG 3698 3703\\tNorth\\n',\n",
       " 'T1\\tORG 3704 3712\\tAtlantic\\n',\n",
       " 'T1\\tORG 3713 3719\\tTreaty\\n',\n",
       " 'T1\\tORG 3720 3732\\tOrganization\\n',\n",
       " 'T1\\tORG 3734 3738\\tNATO\\n',\n",
       " 'T1\\tORG 3741 3753\\tOrganisation\\n',\n",
       " 'T1\\tORG 3754 3757\\tfor\\n',\n",
       " 'T1\\tORG 3758 3766\\tEconomic\\n',\n",
       " 'T1\\tORG 3767 3769\\tCo\\n',\n",
       " 'T1\\tORG 3784 3795\\tDevelopment\\n',\n",
       " 'T1\\tORG 3808 3813\\tWorld\\n',\n",
       " 'T1\\tORG 3814 3819\\tTrade\\n',\n",
       " 'T1\\tORG 3820 3832\\tOrganization\\n',\n",
       " 'T1\\tORG 3834 3837\\tWTO\\n',\n",
       " 'T1\\tORG 3844 3846\\tLa\\n',\n",
       " 'T1\\tORG 3847 3859\\tFrancophonie\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = []\n",
    "idx = 1\n",
    "text = tokenizer(input_data)\n",
    "for sent, label_seq in zip(text.sents, output_client):\n",
    "    for token, label in zip(sent, label_seq):\n",
    "        if (label != \"O\") :\n",
    "            new_ann = 'T'+str(idx)+'\\t'+label[2:]+' '+str(token.idx)+' '+str(token.idx+len(token.string.strip()))+'\\t'+token.string.strip()+'\\n'\n",
    "            annotations.append(new_ann)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.042 s\n",
      "Decoding speed: 747.7 st/s\n",
      "[['B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], ['O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-MISC', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-PER', 'O', 'B-ORG', 'O'], ['O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "['B-LOC O B-MISC O O O O O O O O B-LOC I-LOC O B-MISC I-MISC I-MISC O O O O O O O O O O O O O O O O B-LOC O B-LOC I-LOC O O O O O O O O O O\\n', 'O O O O O O O B-LOC O O O B-LOC I-LOC O O B-ORG I-ORG O O B-LOC I-LOC O O O O B-LOC O O B-LOC I-LOC O\\n', 'O O O O B-LOC I-LOC O B-LOC I-LOC O O O O O B-LOC O B-LOC O B-MISC O O\\n', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O O O O O\\n', 'O O O O O B-ORG O B-ORG O B-ORG O B-ORG O B-PER O B-ORG O\\n', 'O O B-LOC I-LOC O O O O O B-LOC O O O O B-PER O O B-ORG O O\\n', 'B-LOC O O O O O B-MISC O O O O O O O B-PER I-PER O O O O O O B-LOC O B-LOC O\\n', 'B-LOC O O O O B-MISC O O O B-LOC I-LOC I-LOC O O O O O B-MISC I-MISC I-MISC I-MISC O O O O O O\\n', 'O O B-PER O B-MISC O O O O O O O O O O O O O O O O O O O O O O O O\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2gold = '../conll2003/test.conll2003'\n",
    "path2decode = '../pretrained/myModel/myModel.out'\n",
    "path2dset = '../pretrained/myModel/myModel.dset'\n",
    "path2model = '../pretrained/myModel/myModel.0.model'\n",
    "\n",
    "conf_dict = {'raw_dir':path2gold,\n",
    "                   'decode_dir':path2decode,\n",
    "                    'dset_dir':path2dset,\n",
    "                    'load_model_dir':path2model,\n",
    "                    'number_normalized': True\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3683\n",
      "3683\n"
     ]
    }
   ],
   "source": [
    "from utils.data import Data\n",
    "from ner_model import load_model_decode\n",
    "\n",
    "data = Data()\n",
    "data.read_config(conf_dict)\n",
    "data.load(data.dset_dir)\n",
    "data.read_config(conf_dict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data.generate_instance('raw')\n",
    "print(len(data.raw_texts))\n",
    "print(len(data.test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model from file ../pretrained/myModel/myModel\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n",
      "gold_num = 5596; predict_num = 5696; right_num = 4683\n",
      "raw: time10.67s, speed: 349.10st/s; acc: 0.9656, p: 0.8222, r: 0.8368, f: 0.8294\n"
     ]
    }
   ],
   "source": [
    "decode_results, _ = load_model_decode(data, 'raw', label_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(decode_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict raw result has been written into file ../pretrained/myModel/myModel.out\n"
     ]
    }
   ],
   "source": [
    "data.write_decoded_results(decode_results, 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
