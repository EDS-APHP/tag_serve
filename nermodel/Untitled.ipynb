{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.data import Data\n",
    "from ner_model import build_model, evaluate\n",
    "import time\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(data):\n",
    "    ''' align a dictionnary of sequences \n",
    "        input:\n",
    "            data, dict of sequences { 'key1': ['I', 'dream', 'of', 'the', 'Moon'] \n",
    "                        'key2': [O, O, O, O, 'B-LOC']}\n",
    "        output:\n",
    "\n",
    "            dict of strings {'key1': ['I dream of the Moon'] \n",
    "                             'key2': ['O O     O  O   B-LOC']}\n",
    "    '''\n",
    "    spacings = [max([len(seq[i]) for seq in data.values()]) for i in range(len(data[list(data.keys())[0]]))]\n",
    "\n",
    "    data_aligned = dict()\n",
    "\n",
    "    for key, seq in data.items():\n",
    "        str_aligned = \"\"\n",
    "        for token, spacing in zip(seq, spacings):\n",
    "            str_aligned += token + \" \"*(spacing-len(token) + 1)\n",
    "        data_aligned[key] = str_aligned\n",
    "    return data_aligned\n",
    "#print(align_data(align)['inputs'])\n",
    "#print(align_data(align)['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER MODEL: decoding-style loading..\n",
      "Load Model weights from file ../pretrained/baseline.model\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n"
     ]
    }
   ],
   "source": [
    "path2xpt = '../pretrained/baseline.xpt'\n",
    "path2model = '../pretrained/baseline.model'\n",
    "decode_config_dict = {'load_model_dir':path2model # load model file\n",
    "                     }\n",
    "data = Data()\n",
    "#data.read_config(decode_config_dict)\n",
    "print(\"NER MODEL: decoding-style loading..\")\n",
    "## dset_dir must only contains dictionnary informations here (dset from the original model should be cleaned with the function clean_dset (to be coded))\n",
    "data.load_export(path2xpt)\n",
    "## supplementary configurations (optional, maybe not useful in deployment)\n",
    "data.read_config(decode_config_dict)\n",
    "## !!! we should be loading the weights here and not at each prediction!!!!\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "#data.show_data_summary()\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../prod_data/wiki_en_france.txt'\n",
    "out_folder = 'proprecessed/'\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "path2write = out_folder + os.path.basename(os.path.splitext(file_name)[0]) + '.out'\n",
    "# open and return the text of the file\n",
    "with open(file_name, 'r') as f:\n",
    "    input_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_data = 'I am working at the APHP. They have recently refused Google and Facebook cooperation. Camus wrote such beautiful plays'\n",
    "## Pre-processing from client \n",
    "sentences = nltk.sent_tokenize(input_data)\n",
    "input_client = []\n",
    "input_model = []\n",
    "for sent in sentences:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    # we have to keep a sequence wo '' sentences separators for the client output\n",
    "    input_client += tokens\n",
    "    input_model += tokens + ['']\n",
    "#print(input_client)\n",
    "#print(input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time 0.042 s\n",
      "Decoding speed: 747.7 st/s\n",
      "[['B-LOC', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'I-LOC', 'O'], ['O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-MISC', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-PER', 'O', 'B-ORG', 'O'], ['O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'B-ORG', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "['B-LOC O B-MISC O O O O O O O O B-LOC I-LOC O B-MISC I-MISC I-MISC O O O O O O O O O O O O O O O O B-LOC O B-LOC I-LOC O O O O O O O O O O\\n', 'O O O O O O O B-LOC O O O B-LOC I-LOC O O B-ORG I-ORG O O B-LOC I-LOC O O O O B-LOC O O B-LOC I-LOC O\\n', 'O O O O B-LOC I-LOC O B-LOC I-LOC O O O O O B-LOC O B-LOC O B-MISC O O\\n', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\\n', 'O O O B-LOC O O O O O O O O O B-LOC O O O O O O O O O O O O O\\n', 'O O O O O B-ORG O B-ORG O B-ORG O B-ORG O B-PER O B-ORG O\\n', 'O O B-LOC I-LOC O O O O O B-LOC O O O O B-PER O O B-ORG O O\\n', 'B-LOC O O O O O B-MISC O O O O O O O B-PER I-PER O O O O O O B-LOC O B-LOC O\\n', 'B-LOC O O O O B-MISC O O O B-LOC I-LOC I-LOC O O O O O B-MISC I-MISC I-MISC I-MISC O O O O O O\\n', 'O O B-PER O B-MISC O O O O O O O O O O O O O O O O O O O O O O O O\\n']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#print(feed_data)\n",
    "### self.fix_alphabet() placed inside generate_instance* should prevent the vocabularies to grow indefinitely with fed inputs\n",
    "data.generate_instance_from_list(input_model)\n",
    "#print('***************')\n",
    "#print(data.raw_texts)\n",
    "#print(evaluate(data, model, 'raw', label_flag=False))\n",
    "#print('*****************')\n",
    "speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, 'raw', label_flag=False) \n",
    "\n",
    "timed = time.time() - start_time\n",
    "print('Processing time {:.2} s'.format(timed))\n",
    "print('Decoding speed: {0:.1f} st/s'.format(speed))\n",
    "print(pred_results)\n",
    "# reconstruct a unique sequence for the client\n",
    "#output_client = []\n",
    "#for l in pred_results:\n",
    "#    output_client += l\n",
    "\n",
    "#output_aligned = align_data({'raw_input': input_client, 'labels':output_client})\n",
    "#print(output_aligned['raw_input'])\n",
    "#print(output_aligned['labels'])\n",
    "out = [' '.join(sent) +'\\n' for sent in pred_results]\n",
    "print(out)\n",
    "with open(path2write, 'w') as f:\n",
    "    f.writelines(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2gold = '../conll2003/test.conll2003'\n",
    "path2decode = '../pretrained/myModel/myModel.out'\n",
    "path2dset = '../pretrained/myModel/myModel.dset'\n",
    "path2model = '../pretrained/myModel/myModel.0.model'\n",
    "\n",
    "conf_dict = {'raw_dir':path2gold,\n",
    "                   'decode_dir':path2decode,\n",
    "                    'dset_dir':path2dset,\n",
    "                    'load_model_dir':path2model,\n",
    "                    'number_normalized': True\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3683\n",
      "3683\n"
     ]
    }
   ],
   "source": [
    "from utils.data import Data\n",
    "from ner_model import load_model_decode\n",
    "\n",
    "data = Data()\n",
    "data.read_config(conf_dict)\n",
    "data.load(data.dset_dir)\n",
    "data.read_config(conf_dict)\n",
    "data.HP_gpu = torch.cuda.is_available()\n",
    "data.generate_instance('raw')\n",
    "print(len(data.raw_texts))\n",
    "print(len(data.test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model from file ../pretrained/myModel/myModel\n",
      "building Network..\n",
      "use crf:  True\n",
      "use_char:  True\n",
      "char feature extractor:  CNN\n",
      "word feature extractor:  LSTM\n",
      "Build word sequence feature extractor: LSTM...\n",
      "Build word representation...\n",
      "Build char sequence feature extractor: CNN..\n",
      "build CRF...\n",
      "gold_num = 5596; predict_num = 5696; right_num = 4683\n",
      "raw: time10.67s, speed: 349.10st/s; acc: 0.9656, p: 0.8222, r: 0.8368, f: 0.8294\n"
     ]
    }
   ],
   "source": [
    "decode_results, _ = load_model_decode(data, 'raw', label_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(decode_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict raw result has been written into file ../pretrained/myModel/myModel.out\n"
     ]
    }
   ],
   "source": [
    "data.write_decoded_results(decode_results, 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
